{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(x, n_class):\n",
    "    \"\"\"One Hot encoding\n",
    "    \n",
    "    parameter\n",
    "    ---------\n",
    "    x: array_like [n_sample]\n",
    "    \n",
    "    n_class: number of class\n",
    "    \n",
    "    return\n",
    "    ------\n",
    "    en_1hot: array_like [n_smaple, n_class] one hot encoding matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    en_1hot = np.zeros([len(x), n_class])\n",
    "    \n",
    "    for idx, cat in enumerate(x):\n",
    "        en_1hot[idx, cat] = 1\n",
    "    \n",
    "    return en_1hot\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return  exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "def foward(X, params):\n",
    "    \"\"\"Neuron network forward\n",
    "    \n",
    "    parameter\n",
    "    ----------\n",
    "    X: array_like [n_sample, n_input]\n",
    "    params: dictionary of layer weights\n",
    "    \n",
    "    return\n",
    "    ------\n",
    "    output: final layer output\n",
    "    caches: dictionary of foward layer output\n",
    "    \"\"\"\n",
    "    \n",
    "    W1, b1 = params['W1'], params['b1']\n",
    "    W2, b2 = params['W2'], params['b2']\n",
    "    \n",
    "    Z1 = np.matmul(X, W1) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.matmul(A1, W2) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    caches = {'Z1': Z1, 'A1': A1,\n",
    "              'Z2': Z2, 'A2': A2}\n",
    "\n",
    "    return A2, caches\n",
    "    \n",
    "\n",
    "def backward(X, y, params, caches):\n",
    "    \"\"\"Neuron network backward\n",
    "    \n",
    "    parameter\n",
    "    ----------\n",
    "    X: array_like [n_sample, n_input]\n",
    "    y: array_like [n_sample, n_class]\n",
    "    params: dictionary of layer weights\n",
    "    caches: dictionary of foward layer output\n",
    "    \n",
    "    return\n",
    "    ------\n",
    "    grads: dictionary of layer weights gradient\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = params['W1']\n",
    "    W2 = params['W2']\n",
    "\n",
    "    A1 = caches['A1']\n",
    "    A2 = caches['A2']\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # chain rule compute gradient\n",
    "    # last layer gradient\n",
    "    dZ2 = A2 - y\n",
    "    dW2 = np.matmul(A1.T, dZ2) / m\n",
    "    db2 = np.sum(dZ2, axis=0) / m\n",
    "    \n",
    "    dZ1 = np.multiply(np.matmul(dZ2, W2.T), (np.multiply(A1 ,(1 - A1))))\n",
    "    dW1 = np.matmul(X.T, dZ1) / m\n",
    "    db1 = np.sum(dZ1, axis=0) / m\n",
    "    \n",
    "    grads = {'dW1': dW1, 'db1': db1,\n",
    "             'dW2': dW2, 'db2': db2}\n",
    "    \n",
    "    return grads\n",
    "\n",
    "\n",
    "def update_parameters(params, grads, learning_rate):\n",
    "    \"\"\"Update parameters\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    grads: dictionary of layer weights gradient\n",
    "    params: dictionary of layer weights\n",
    "    learning_rate: learing rate to update weights\n",
    "    \n",
    "    \"\"\"\n",
    "    params['W1'] = params['W1'] - learning_rate * grads['dW1']\n",
    "    params['b1'] = params['b1'] - learning_rate * grads['db1']\n",
    "    params['W2'] = params['W2'] - learning_rate * grads['dW2']\n",
    "    params['b2'] = params['b2'] - learning_rate * grads['db2']\n",
    "    \n",
    "    return params\n",
    "\n",
    "\n",
    "def compute_cost(output, y):\n",
    "    \"\"\"Compute loss\n",
    "    \n",
    "    parameter\n",
    "    ---------\n",
    "    output: array_like [n_sample, n_class] nn predict output\n",
    "    y: array_like [n_sample, n_class]\n",
    "    \"\"\"\n",
    "    loss = np.multiply(y, np.log(output)) + np.multiply((1 - y), np.log(1 - output))\n",
    "    loss = -np.mean(np.sum(loss, axis=-1, keepdims=True))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, params, learning_rate=0.01, max_iter=None, tol=1e-4,\n",
    "                     show_cost=None):\n",
    "    \"\"\"Gradient descent \n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    X: array_like [n_samples, n_features]\n",
    "    y: array_like [n_samples, n_class]\n",
    "    params: dictionary of layer weights\n",
    "\n",
    "    return\n",
    "    ------\n",
    "    parmas: dictionary of layer weights\n",
    "    costs: costs of iteratsions\n",
    "    \"\"\"\n",
    "    \n",
    "    if max_iter is None:\n",
    "        max_iter = 1000\n",
    "    \n",
    "    costs = np.zeros(max_iter)\n",
    "    \n",
    "    for n_iter in np.arange(0, max_iter):    \n",
    "        \n",
    "        output, caches = foward(X, params)\n",
    "        costs[n_iter] = compute_cost(output, y)\n",
    "        grads = backward(X, y, params, caches)\n",
    "        params = update_parameters(params, grads, learning_rate)\n",
    "        \n",
    "        if show_cost is not None and (n_iter % show_cost) == 0:\n",
    "            print('costs[%4d]: %.4e' % (n_iter, costs[n_iter]))\n",
    "            \n",
    "        if np.abs(costs[n_iter - 1] - costs[n_iter]) < tol:\n",
    "            costs = costs[:n_iter]\n",
    "            break\n",
    "    \n",
    "    return params, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_split_data(csv_file):\n",
    "    \"\"\"Split image and label from mnist csv data\n",
    "    \n",
    "    return\n",
    "    ------\n",
    "    X: array_like [n_sample, n_feature] mnist flat image\n",
    "    y: array_like [n_sample] mnist image label\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    X = df.iloc[:, :-1].values\n",
    "    y = df.iloc[:, -1].values\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = mnist_split_data('ex3_train.csv')\n",
    "X_test, y_test = mnist_split_data('ex3_test.csv')\n",
    "\n",
    "n_class = len(np.unique(y_train))\n",
    "y_train_en = one_hot_encode(y_train, n_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(layer_dims):\n",
    "    \"\"\"weights initialize for layers\n",
    "    \n",
    "    parameter\n",
    "    ---------\n",
    "    layer_dims: list [n_input, n_output]\n",
    "    \n",
    "    return\n",
    "    ------\n",
    "    model: dictionary of layer weights\n",
    "    \"\"\"\n",
    "    \n",
    "    model = {}\n",
    "    \n",
    "    for idx, (n_input, n_output) in enumerate(layer_dims):\n",
    "        \n",
    "        layer_weights = 'W{:d}'.format(idx + 1)\n",
    "        layer_biases = 'b{:d}'.format(idx + 1)\n",
    "        \n",
    "        weights = np.multiply(np.random.randn(n_input, n_output), 0.01)\n",
    "        b = np.zeros([n_output])\n",
    "        \n",
    "        model[layer_weights] = weights\n",
    "        model[layer_biases] = b\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Network model with 1 hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [(400, 25), \n",
    "              (25,  10)]\n",
    "\n",
    "model = weights_init(layer_dims)\n",
    "model, costs = gradient_descent(X_train, y_train_en, model, tol=1e-6,\n",
    "                                 learning_rate=0.1, max_iter=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, model):\n",
    "    \"\"\"Predcit smaples with softmax prob\n",
    "    \n",
    "    parameter\n",
    "    ---------\n",
    "    X: array_like [n_sample, n_feature]\n",
    "    model: dictionary of layer weights\n",
    "    \n",
    "    return\n",
    "    ------\n",
    "    softmax prob label \n",
    "    \"\"\"\n",
    "    \n",
    "    output, _ = foward(X, model)\n",
    "    \n",
    "    return np.argmax(softmax(output), axis=-1)\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    \n",
    "    return np.sum(np.equal(y_pred, y_true)) / len(y_true)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = predict(X_train, model)\n",
    "train_acc = accuracy(y_train_pred, y_train)\n",
    "print(\"train acc: {:6.4f}\".format(train_acc * 100))\n",
    "\n",
    "y_test_pred = predict(X_test, model)\n",
    "test_acc = accuracy(y_test_pred, y_test)\n",
    "print(\"test acc: {:6.4f}\".format(train_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyper parameter tuning grid search\n",
    "n_iters = [2000, 3000, 6000]\n",
    "#n_iters = [100, 300, 500]\n",
    "learning_rates = [0.01, 0.1, 1]\n",
    "grid = [(learning_rate, n_iter) for learning_rate in learning_rates for n_iter in n_iters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [(400, 25), \n",
    "              (25,  10)]\n",
    "\n",
    "for idx, (learning_rate, n_iter) in enumerate(grid):\n",
    "    model = weights_init(layer_dims)\n",
    "    model, costs = gradient_descent(X_train, y_train_en, model, tol=1e-10,\n",
    "                                    learning_rate=learning_rate, max_iter=n_iter)\n",
    "    y_train_pred = predict(X_train, model)\n",
    "    train_acc = accuracy(y_train_pred, y_train)   \n",
    "    \n",
    "    y_test_pred = predict(X_test, model)\n",
    "    test_acc = accuracy(y_test_pred, y_test)\n",
    "    \n",
    "    plt.xlabel('iteraions')\n",
    "    plt.ylabel('cost')\n",
    "    plt.xticks(np.arange(0, n_iter, n_iter / 10))\n",
    "    plt.xlim(xmin=1, xmax=n_iter)\n",
    "    plt.plot(costs)\n",
    "    plt.show()\n",
    "    print('learning rate {:.2f} iters {:d}'.format(learning_rate, n_iter))\n",
    "    print('Train acc: {:6.4f}'.format(train_acc * 100))\n",
    "    print('Test acc: {:6.4f}'.format(test_acc * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
