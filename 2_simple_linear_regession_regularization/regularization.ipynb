{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple linear regession regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta\n",
    "import six\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import RegressorMixin\n",
    "from sklearn.linear_model.base import LinearModel\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "# set random seed to have consistent result\n",
    "np.random.seed(1)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(target, pred):\n",
    "    \"\"\"Compute RMSE between predict and true target\n",
    "    \"\"\"\n",
    "    return np.sqrt(mean_squared_error(target, pred))\n",
    "\n",
    "\n",
    "def create_poly_feature_pipe(degree, normalize=True):\n",
    "    \"\"\"Creaet poly feature transform pipeline\n",
    "    \"\"\"\n",
    "    poly = PolynomialFeatures(degree, include_bias=False)\n",
    "    \n",
    "    if normalize:\n",
    "        scaler = StandardScaler()\n",
    "        pipe = make_pipeline(poly, scaler)\n",
    "    else:\n",
    "        pipe = make_pipeline(poly)\n",
    "        \n",
    "    return pipe\n",
    "        \n",
    "\n",
    "def extract_model_weights(model):\n",
    "    \"\"\"Extract weights from model\n",
    "    \"\"\"\n",
    "    if hasattr(model, 'intercept_'):\n",
    "        w = np.zeros(len(model.coef_.ravel()) + 1)\n",
    "        w[0] = model.intercept_\n",
    "        w[1:] = model.coef_.ravel()\n",
    "    else:\n",
    "        w = model.coef_.copy()\n",
    "        \n",
    "    return w\n",
    "    \n",
    "    \n",
    "def print_model_weight(model):    \n",
    "    print('weights:\\n', np.array2string(extract_model_weights(model).ravel(), \n",
    "                        formatter={'float_kind':'{:2.6e}'.format}, max_line_width=80))\n",
    "\n",
    "def print_model_rmse(model, preprocess_pipe, data_set, string_format='RMSE: {:2.6e}'):\n",
    "    X, y  = data_set\n",
    "    X_tr = preprocess_pipe.transform(X)\n",
    "    rmse = compute_rmse(y, model.predict(X_tr))\n",
    "    print(string_format.format(rmse))\n",
    "\n",
    "\n",
    "def show_model_result(model, preprocess_pipe, train_set, valid_set=None, test_set=None):\n",
    "    \"\"\"Print linear regression model RMSE and weights\n",
    "    \n",
    "    :param model: linear regression model\n",
    "    :param preprocess_pipe:  preprocess pipeline \n",
    "    :param train_set: (X, y) tuple train data X, y \n",
    "    :param valid_set: (X, y) tuple valid data X, y\n",
    "    :param test_set:  (X, y) tuple test  data X, y\n",
    "    \"\"\"\n",
    "    \n",
    "    print_model_rmse(model, preprocess_pipe, train_set, 'Train RMSE: {:2.6e}')\n",
    "    \n",
    "    if valid_set is not None:\n",
    "        print_model_rmse(model, preprocess_pipe, valid_set, 'Valid RMSE: {:2.6e}')\n",
    "    \n",
    "    if test_set is not None:\n",
    "        print_model_rmse(model, preprocess_pipe, test_set, 'Test RMSE: {:2.6e}')\n",
    "    \n",
    "    print_model_weight(model)\n",
    "    print('='*80)\n",
    "\n",
    "\n",
    "def plot_model_result(model, preprocess_pipe, train_set, valid_set=None, test_set=None):\n",
    "    \"\"\"Plot linear regression model result\n",
    "    \"\"\"\n",
    "    \n",
    "    def plot_data(data_set, color, label):\n",
    "        X, y = data_set\n",
    "        plt.scatter(X, y, color=color, s=20, alpha=0.5, label=label)\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))\n",
    "        \n",
    "    plot_data(train_set, color='b', label='Train')\n",
    "\n",
    "    if valid_set is not None:\n",
    "        plot_data(valid_set, color='g', label='Valid')\n",
    "        \n",
    "    if test_set is not None:\n",
    "        plot_data(test_set, color='y', label='Test')\n",
    "    \n",
    "    train_X, train_y = train_set\n",
    "    plot_X = np.linspace(train_X.min(), train_X.max(), 1000).reshape([-1, 1])\n",
    "    plot_X_tr = preprocess_pipe.transform(plot_X)\n",
    "    plot_y_pred = model.predict(plot_X_tr)\n",
    "    plt.plot(plot_X, plot_y_pred, label='model', linewidth=3, color='r')\n",
    "    \n",
    "    plt.title('lambda: {:.2e}'.format(model.alpha))\n",
    "    plt.xlim(xmin=train_X.min(), xmax=train_X.max())\n",
    "    plt.ylim(ymin=train_y.min(), ymax=train_y.max())\n",
    "    plt.xlabel('X', fontsize='large')\n",
    "    plt.ylabel('y', rotation='horizontal', fontsize='large')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression cost and gradient function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, lambda_):\n",
    "    \"\"\"Linear regression cost function with l2-norm regularization\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    y_hat = X @ w\n",
    "    diff = y_hat - y\n",
    "    \n",
    "    ww = w.copy()\n",
    "    ww[0] = 0\n",
    "    cost = (diff.T @ diff + lambda_ * ww.T @ ww) / (2 * n_samples) \n",
    "    \n",
    "    return cost\n",
    "\n",
    "    \n",
    "def compute_grad(X, y, w, lambda_):\n",
    "    \"\"\"Linear regression grad function with l2-nrom regularization\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    y_hat = X @ w\n",
    "    diff = y_hat - y\n",
    "\n",
    "    # compute grad\n",
    "    cost_grad = X.T @ diff\n",
    "        \n",
    "    # compute regularization grad\n",
    "    mask = np.ones([n_features, 1])\n",
    "    mask[0] = 0\n",
    "    reg_grad = lambda_ * (w * mask) \n",
    "    grad = (cost_grad + reg_grad) / n_samples\n",
    "          \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _BaseSolver(six.with_metaclass(ABCMeta, LinearModel)):\n",
    "    def predict(self, X, y=None):\n",
    "        X, y = preprocess_data(X, y, self.add_bias)\n",
    "        return X @ self.coef_\n",
    "\n",
    "#    def rmse(self, X, y):\n",
    "#        return rmse(self.predict(X) - y)\n",
    "#    \n",
    "#    def cost(self, X, y):\n",
    "#        return compute_cost(y, self.predict(X))\n",
    "\n",
    "\n",
    "def preprocess_data(X, y, add_bias):\n",
    "    \"\"\"Preprocess X, y data with add bias term to X\n",
    "    \n",
    "    :param X: {n_samples, n_featurs}\n",
    "    :param y: {n_samples}\n",
    "    :param add_bias: if True add bias term to X\n",
    "    :return: X, y\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "    \n",
    "    if add_bias:\n",
    "        bias = np.ones([X.shape[0], 1])\n",
    "        X = np.hstack([bias, X])\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_solver(X, y, lambda_=1, learning_rate=1e-2, tol=1e-8, max_iter=None):\n",
    "    \"\"\"Solve Xw = y using gradient desecent method with l2-norm regularization\n",
    "    \n",
    "    :param X: {n_samples, n_featurs}\n",
    "    :param y: {n_samples} \n",
    "    :param lambda_: l2-norm regularization penalty\n",
    "    :param learning_rate: gradent step \n",
    "    :param tol: tolerance between cost funciton\n",
    "    :param max_iter: max iterations number\n",
    "    :return: {n_samples} coffecient\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros([n_features, 1])\n",
    "    \n",
    "    mask = np.ones([n_features, 1])\n",
    "    mask[0] = 0\n",
    "    \n",
    "    if max_iter is None:\n",
    "        max_iter = 1000\n",
    "    costs = np.zeros(max_iter)\n",
    "    \n",
    "    costs[0] = compute_cost(X, y, w, lambda_)\n",
    "    for n_iter in np.arange(1, max_iter):    \n",
    "        y_hat = X @ w\n",
    "        diff = y_hat - y\n",
    "    \n",
    "        grad = compute_grad(X, y, w, lambda_)\n",
    "        w = w - learning_rate * grad\n",
    "        \n",
    "        costs[n_iter] = compute_cost(X, y, w, lambda_)\n",
    "        \n",
    "        #print('costs[%4d]: %.8f' % (n_iter, costs[n_iter]))\n",
    "        if np.abs(costs[n_iter - 1] - costs[n_iter]) < tol:\n",
    "            costs = costs[:n_iter]\n",
    "            break\n",
    "    \n",
    "    return w, n_iter, costs\n",
    "    \n",
    "\n",
    "# from scipy.optimize import fmin_cg\n",
    "# \n",
    "# class CGSolver(_BaseSolver, RegressorMixin):\n",
    "#     \"\"\"USing scipy.optimize.fmin_cg to optimize linear regression model\n",
    "#     \"\"\"\n",
    "#     def __init__(self, alpha=1.0, add_bias=True, max_iter=None, tol=1e-6):\n",
    "#         self.alpha = alpha\n",
    "#         self.add_bias = add_bias\n",
    "#         self.max_iter = max_iter\n",
    "#         self.tol = tol\n",
    "#         \n",
    "#     def fit(self, X, y):\n",
    "#         X, y = preprocess_data(X, y, self.add_bias)\n",
    "# \n",
    "#         def cost_fun(w, X, y, lambda_):\n",
    "#             w = np.reshape(w, [-1, 1])\n",
    "#             return compute_cost(X, y, w, lambda_)\n",
    "#         \n",
    "#         def grad_fun(w, X, y, lambda_):\n",
    "#             w = np.reshape(w, [-1, 1])\n",
    "#             grad = compute_grad(X, y, w, lambda_)\n",
    "#             \n",
    "#             return grad.flatten()\n",
    "#         \n",
    "#         x0 = np.zeros([X.shape[1]])\n",
    "#         self.coef_ = \\\n",
    "#             fmin_cg(cost_fun, x0, fprime=grad_fun, args=(X, y, self.alpha), \n",
    "#                     maxiter=self.max_iter, epsilon=self.tol,\n",
    "#                     disp=True)\n",
    "#         self.coef_ = self.coef_.T\n",
    "        \n",
    "    \n",
    "class GradientDescentSolver(_BaseSolver, RegressorMixin):\n",
    "    \"\"\"Gradient descent solver with l2 regularization\n",
    "    \n",
    "    This model solves a regression model with the loss function using\n",
    "    gradient descent method with l2-nrom regularization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.0, add_bias=True, learning_rate=1e-2, max_iter=None, tol=1e-8):\n",
    "        self.alpha = alpha\n",
    "        self.add_bias = add_bias\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X, y = preprocess_data(X, y, self.add_bias)\n",
    "        \n",
    "        self.coef_, self.n_iters_, self.costs_ = \\\n",
    "            gradient_descent_solver(X, y, \n",
    "                                    lambda_=self.alpha, learning_rate=self.learning_rate,\n",
    "                                    max_iter=self.max_iter, tol=self.tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Polynomial regression / overfitting / regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the data using linear (1st order) regression model (gradient descent method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "df = pd.read_csv('ex2data1.csv')\n",
    "data1_X = df.iloc[:, :-1].values.reshape([-1, 1])\n",
    "data1_y = df.iloc[:, -1].values.reshape([-1, 1])\n",
    "\n",
    "data1_train_set = (data1_X, data1_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "degree = 1\n",
    "lambda_ = 0\n",
    "\n",
    "# create preprocess pipe line\n",
    "preprocess_pipe = create_poly_feature_pipe(degree, normalize=True)\n",
    "data1_X_tr = preprocess_pipe.fit_transform(data1_X)\n",
    "    \n",
    "model = GradientDescentSolver(lambda_)\n",
    "model.fit(data1_X_tr, data1_y)\n",
    "\n",
    "plot_model_result(model, preprocess_pipe, data1_train_set)\n",
    "show_model_result(model, preprocess_pipe, data1_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the data using 2nd order polynomial regression model (gradient descent method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "degree = 2\n",
    "lambda_ = 0\n",
    "\n",
    "# create preprocess pipe line\n",
    "preprocess_pipe = create_poly_feature_pipe(degree, normalize=True)\n",
    "data1_X_tr = preprocess_pipe.fit_transform(data1_X)\n",
    "    \n",
    "model = GradientDescentSolver(lambda_)\n",
    "model.fit(data1_X_tr, data1_y)\n",
    "\n",
    "plot_model_result(model, preprocess_pipe, data1_train_set)\n",
    "show_model_result(model, preprocess_pipe, data1_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the data using 4th order polynomial regression model (gradient descent method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "degree = 4\n",
    "lambda_ = 0\n",
    "\n",
    "# create preprocess pipe line\n",
    "preprocess_pipe = create_poly_feature_pipe(degree, normalize=True)\n",
    "data1_X_tr = preprocess_pipe.fit_transform(data1_X)\n",
    "    \n",
    "model = GradientDescentSolver(lambda_, learning_rate=0.1, max_iter=8000)\n",
    "model.fit(data1_X_tr, data1_y)\n",
    "\n",
    "plot_model_result(model, preprocess_pipe, data1_train_set)\n",
    "show_model_result(model, preprocess_pipe, data1_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the data using 16th order polynomial regression model (gradient descent method).\n",
    "\n",
    "With 16 degree poly features, but only have very small sample data (aka a under-fitting system)\n",
    "\n",
    "This caused matrix ***X*** very ill-conditioned.\n",
    "\n",
    "Without L2-norm regularization (lambda_ = 0), matrix form result is very unstable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "degree = 16\n",
    "lambda_ = 0\n",
    "\n",
    "# create preprocess pipe line\n",
    "preprocess_pipe = create_poly_feature_pipe(degree, normalize=True)\n",
    "data1_X_tr = preprocess_pipe.fit_transform(data1_X)\n",
    "    \n",
    "model = GradientDescentSolver(lambda_, learning_rate=0.1, max_iter=2000)\n",
    "model.fit(data1_X_tr, data1_y)\n",
    "\n",
    "plot_model_result(model, preprocess_pipe, data1_train_set)\n",
    "show_model_result(model, preprocess_pipe, data1_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the data using 16th order polynomial regression model with ridge (L2 penalty) regularization (gradient descent method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "degree = 16\n",
    "\n",
    "# create preprocess pipe line\n",
    "preprocess_pipe = create_poly_feature_pipe(degree, normalize=True)\n",
    "data1_X_tr = preprocess_pipe.fit_transform(data1_X)\n",
    "    \n",
    "lambda_vec = [np.power(10.0, p) for p in np.arange(-1, 2, 1)]\n",
    "\n",
    "for lambda_ in lambda_vec:\n",
    "\n",
    "    model = GradientDescentSolver(lambda_, learning_rate=0.1, max_iter=3000)\n",
    "    model.fit(data1_X_tr, data1_y)\n",
    "\n",
    "    plot_model_result(model, preprocess_pipe, data1_train_set)\n",
    "    show_model_result(model, preprocess_pipe, data1_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the data using 16th order polynomial regression model with scikit-learn Ridge model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# parameters\n",
    "degree = 16\n",
    "\n",
    "# create preprocess pipe line\n",
    "preprocess_pipe = create_poly_feature_pipe(degree, normalize=True)\n",
    "data1_X_tr = preprocess_pipe.fit_transform(data1_X)\n",
    "\n",
    "lambda_vec = [np.power(10.0, p) for p in np.arange(-1, 2, 1)]\n",
    "\n",
    "for lambda_ in lambda_vec:\n",
    "\n",
    "    model = Ridge(lambda_)\n",
    "    model.fit(data1_X_tr, data1_y)\n",
    "\n",
    "    plot_model_result(model, preprocess_pipe, data1_train_set)\n",
    "    show_model_result(model, preprocess_pipe, data1_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the data using 16th order polynomial regression model with scikit-learn Lasso model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1-norm regularization tend to sparse solution, so the most weights compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# create preprocess pipe line\n",
    "preprocess_pipe = create_poly_feature_pipe(degree, normalize=True)\n",
    "data1_X_tr = preprocess_pipe.fit_transform(data1_X)\n",
    "\n",
    "# parameters\n",
    "degree = 16\n",
    "\n",
    "lambda_vec = [np.power(10.0, p) for p in np.arange(-1, 2, 1)]\n",
    "\n",
    "for lambda_ in lambda_vec:\n",
    "\n",
    "    model = Lasso(lambda_)\n",
    "    model.fit(data1_X_tr, data1_y)\n",
    "\n",
    "    plot_model_result(model, preprocess_pipe, data1_train_set)\n",
    "    show_model_result(model, preprocess_pipe, data1_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Polynomial regression with train/validation/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(data, train_ratio, valid_ratio, test_ratio):\n",
    "    \"\"\"Split with ratio\n",
    "    \"\"\"\n",
    "    shuffle_idx = np.random.permutation(len(data))\n",
    "    \n",
    "    train_size = int(len(data) * train_ratio)\n",
    "    valid_size = int(len(data) * valid_ratio)\n",
    "    test_size  = int(len(data) * test_ratio)\n",
    "    \n",
    "    train_idx = shuffle_idx[:train_size]\n",
    "    valid_idx = shuffle_idx[(train_size):(train_size + valid_size)]\n",
    "    test_idx  = shuffle_idx[(train_size + valid_size):]\n",
    "\n",
    "    return data.iloc[train_idx], data.iloc[valid_idx], data.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ex2data2.csv')\n",
    "split_train, split_valid, split_test = train_val_test_split(df, 0.6, 0.2, 0.2)\n",
    "\n",
    "data2_train_X, data2_train_y = split_train.iloc[:, 0].values.reshape(-1, 1), \\\n",
    "                               split_train.iloc[:, -1].values.reshape(-1, 1)\n",
    "data2_valid_X, data2_valid_y = split_valid.iloc[:, 0].values.reshape(-1, 1), \\\n",
    "                               split_valid.iloc[:, -1].values.reshape(-1, 1)\n",
    "data2_test_X ,  data2_test_y = split_test.iloc[:, 0].values.reshape(-1, 1), \\\n",
    "                               split_test.iloc[:, -1].values.reshape(-1, 1)\n",
    "\n",
    "data2_train_set = (data2_train_X, data2_train_y)\n",
    "data2_valid_set = (data2_valid_X, data2_valid_y)\n",
    "data2_test_set  = (data2_test_X ,  data2_test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the data using linear (1st order) regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "degree = 1\n",
    "lambda_ = 0\n",
    "\n",
    "# create preprocess pipe line\n",
    "preprocess_pipe = create_poly_feature_pipe(degree, normalize=True)\n",
    "data2_train_X_tr = preprocess_pipe.fit_transform(data2_train_X)\n",
    "    \n",
    "model = GradientDescentSolver(lambda_)\n",
    "model.fit(data2_train_X_tr, data2_train_y)\n",
    "\n",
    "plot_model_result(model, preprocess_pipe, data2_train_set)\n",
    "show_model_result(model, preprocess_pipe, data2_train_set, test_set=data2_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the data using 2nd order polynomial regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "degree = 2\n",
    "lambda_ = 0\n",
    "\n",
    "# create preprocess pipe line\n",
    "preprocess_pipe = create_poly_feature_pipe(degree, normalize=True)\n",
    "data2_train_X_tr = preprocess_pipe.fit_transform(data2_train_X)\n",
    "    \n",
    "model = GradientDescentSolver(lambda_)\n",
    "model.fit(data2_train_X_tr, data2_train_y)\n",
    "\n",
    "plot_model_result(model, preprocess_pipe, data2_train_set)\n",
    "show_model_result(model, preprocess_pipe, data2_train_set, test_set=data2_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the data using 4th order polynomial regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "degree = 4\n",
    "lambda_ = 0\n",
    "\n",
    "# create preprocess pipe line\n",
    "preprocess_pipe = create_poly_feature_pipe(degree, normalize=True)\n",
    "data2_train_X_tr = preprocess_pipe.fit_transform(data2_train_X)\n",
    "    \n",
    "model = GradientDescentSolver(lambda_)\n",
    "model.fit(data2_train_X_tr, data2_train_y)\n",
    "\n",
    "plot_model_result(model, preprocess_pipe, data2_train_set)\n",
    "show_model_result(model, preprocess_pipe, data2_train_set, test_set=data2_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the data using 16th order polynomial regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "degree = 16\n",
    "lambda_ = 0\n",
    "\n",
    "# create preprocess pipe line\n",
    "preprocess_pipe = create_poly_feature_pipe(degree, normalize=True)\n",
    "data2_train_X_tr = preprocess_pipe.fit_transform(data2_train_X)\n",
    "    \n",
    "model = GradientDescentSolver(lambda_)\n",
    "model.fit(data2_train_X_tr, data2_train_y)\n",
    "\n",
    "plot_model_result(model, preprocess_pipe, data2_train_set)\n",
    "show_model_result(model, preprocess_pipe, data2_train_set, test_set=data2_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the data using 16th order polynomial regression model with ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To select optimal $\\lambda$ by compute different rmse for test and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_validate_curve(model, preprocess_pipe, \n",
    "                           train_set, valid_set, lambda_vec):\n",
    "    \"\"\"Compute different lambda_ value affect RMSE for train and valid set\n",
    "    \"\"\"\n",
    "    train_rmse = np.zeros(len(lambda_vec))\n",
    "    valid_rmse = np.zeros(len(lambda_vec))\n",
    "    weights = []\n",
    "    \n",
    "    train_X, train_y = train_set\n",
    "    valid_X, valid_y = valid_set\n",
    "    \n",
    "    train_X_tr = preprocess_pipe.transform(train_X)\n",
    "    valid_X_tr = preprocess_pipe.transform(valid_X)\n",
    "    \n",
    "    for idx, lambda_ in enumerate(lambda_vec):\n",
    "        \n",
    "        # dirty hack to chage model regularization penalty\n",
    "        model.alpha = lambda_\n",
    "        model.fit(train_X_tr, train_y)\n",
    "        weights.append(extract_model_weights(model))\n",
    "        \n",
    "        train_rmse[idx] = compute_rmse(train_y, model.predict(train_X_tr))\n",
    "        valid_rmse[idx] = compute_rmse(valid_y, model.predict(valid_X_tr))\n",
    "    \n",
    "    weights = np.squeeze(np.array(weights))\n",
    "    \n",
    "    return train_rmse, valid_rmse, weights\n",
    "\n",
    "def plot_validate_curve(train_rmse, valid_rmse, lambda_vec):\n",
    "    \"\"\"Plot different lambda on train and valid result\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(lambda_vec, train_rmse, color='g', alpha=0.5, label='Train')\n",
    "    plt.plot(lambda_vec, valid_rmse, color='b', alpha=0.5, label='Valid')\n",
    "    plt.title('$\\lambda$ curve')\n",
    "    plt.xlim(xmin=lambda_vec.min(), xmax=lambda_vec.max())\n",
    "    plt.ylim(ymax=max(train_rmse.max(), valid_rmse.max()))\n",
    "    plt.xlabel('$\\lambda$', fontsize='large')\n",
    "    plt.ylabel('RMSE', fontsize='large')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_lambda_weights_effect(weights, lambda_vec, weights_name):\n",
    "    \"\"\"Plot the weights value change with different lambda\n",
    "    \n",
    "    :param weights: [n_lambda, n_feature] \n",
    "    :param lambda_vec: [n_lambda] \n",
    "    :param weights_name: list of weight name with length n_lambda\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    n_lambda, n_features = weights.shape\n",
    "    \n",
    "    for n in range(n_features):\n",
    "        w = weights[:, n]\n",
    "        label_name = weights_name[n]\n",
    "        plt.plot(lambda_vec, w, label=label_name, alpha=0.7)\n",
    "\n",
    "    plt.title('weights regularization by lambda')\n",
    "    plt.xlabel('$\\lambda$')\n",
    "    plt.ylabel('weight')\n",
    "\n",
    "    plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "degree = 16\n",
    "\n",
    "grid = np.linspace(-2, 5, 50)\n",
    "lambda_vec = np.array([0] + [np.power(10.0, p) for p in grid])\n",
    "#lambda_vec = np.r_[0, np.logspace(-2, 5, 20)].reshape(-1, 1)\n",
    "\n",
    "train_rmse = np.zeros(len(lambda_vec))\n",
    "valid_rmse = np.zeros(len(lambda_vec))\n",
    "\n",
    "# FIXME: GradientDescentSolver get wrong result in compute_validate_curve\n",
    "#preprocess_pipe = create_poly_feature_pipe(degree, normalize=True)\n",
    "#preprocess_pipe.fit(X_train)\n",
    "#model = GradientDescentSolver(learning_rate=0.01, max_iter=8000)\n",
    "#train_rmse, valid_rmse = compute_validate_curve(model, preprocess_pipe, train_set, valid_set, lambdas)\n",
    "#plot_validate_curve(train_rmse, valid_rmse, lambdas)\n",
    "\n",
    "# create preprocess pipe line\n",
    "preprocess_pipe = create_poly_feature_pipe(degree, normalize=True)\n",
    "data2_train_X_tr = preprocess_pipe.fit_transform(data2_train_X)\n",
    "data2_valid_X_tr = preprocess_pipe.transform(data2_valid_X)\n",
    "\n",
    "weights = []\n",
    "for idx, lambda_ in enumerate(lambda_vec):\n",
    "\n",
    "    model = GradientDescentSolver(lambda_, learning_rate=0.01, max_iter=8000)\n",
    "    model.fit(data2_train_X_tr, data2_train_y)\n",
    "    weights.append(extract_model_weights(model))\n",
    "    \n",
    "    train_rmse[idx] = compute_rmse(data2_train_y, model.predict(data2_train_X_tr))\n",
    "    valid_rmse[idx] = compute_rmse(data2_valid_y, model.predict(data2_valid_X_tr))\n",
    "\n",
    "weights = np.squeeze(np.array(weights))\n",
    "\n",
    "plot_validate_curve(train_rmse, valid_rmse, lambda_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent with ridge (L2 penalty) regularization $\\lambda$ on weights effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot weights with lambda regularization\n",
    "weights_name = ['{:2d} th poly'.format(d) for d in np.arange(1, degree + 1)]\n",
    "weights = weights[:, 1:] # remove bias term\n",
    "plot_lambda_weights_effect(weights, lambda_vec, weights_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Decent best $\\lambda$ result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find lambda with lowest train and valid rmse, skip lambda = 0\n",
    "opt_lambda = lambda_vec[np.argmin(np.abs(train_rmse[1:] - valid_rmse[1:])) + 1]\n",
    "data2_train_X_tr = preprocess_pipe.transform(data2_train_X)\n",
    "model.alpha = opt_lambda\n",
    "model.fit(data2_train_X_tr, data2_train_y)\n",
    "\n",
    "print(\"optimal lambda {:2.6e}\".format(opt_lambda))\n",
    "plot_model_result(model, preprocess_pipe, data2_train_set, data2_valid_set)\n",
    "show_model_result(model, preprocess_pipe, data2_train_set, data2_valid_set, data2_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the data using 16th order polynomial regression model with scikit-learn Ridge model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "degree = 16\n",
    "\n",
    "grid = np.linspace(-2, 5, 50)\n",
    "lambda_vec = np.array([0] + [np.power(10.0, p) for p in grid])\n",
    "#lambda_vec = np.r_[0, np.logspace(-2, 5, 20)].reshape(-1, 1)\n",
    "\n",
    "preprocess_pipe = create_poly_feature_pipe(degree, normalize=True)\n",
    "preprocess_pipe.fit(data2_train_X)\n",
    "\n",
    "model = Ridge(solver='lsqr')\n",
    "train_rmse, valid_rmse, weights = compute_validate_curve(model, preprocess_pipe, \n",
    "                                                data2_train_set, data2_valid_set, lambda_vec)\n",
    "plot_validate_curve(train_rmse, valid_rmse, lambda_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scikit-learn Ridge $\\lambda$ on weights effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot weights with lambda regularization\n",
    "weights_name = ['{:2d} th poly'.format(d) for d in np.arange(1, degree + 1)]\n",
    "weights = weights[:, 1:] # remove bias term\n",
    "plot_lambda_weights_effect(weights, lambda_vec, weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find lambda with lowest train and valid rmse, skip lambda = 0\n",
    "opt_lambda = lambda_vec[np.argmin(np.abs(train_rmse[1:] - valid_rmse[1:])) + 1]\n",
    "data2_train_X_tr = preprocess_pipe.transform(data2_train_X)\n",
    "model.alpha = opt_lambda\n",
    "model.fit(data2_train_X_tr, data2_train_y)\n",
    "\n",
    "print(\"optimal lambda {:2.6e}\".format(opt_lambda))\n",
    "plot_model_result(model, preprocess_pipe, data2_train_set, data2_valid_set)\n",
    "show_model_result(model, preprocess_pipe, data2_train_set, data2_valid_set, data2_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the data using 16th order polynomial regression model with scikit-learn Lasso model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "degree = 16\n",
    "\n",
    "grid = np.linspace(-2, 5, 50)\n",
    "lambda_vec = np.array([0] + [np.power(10.0, p) for p in grid])\n",
    "#lambda_vec = np.r_[0, np.logspace(-2, 5, 20)].reshape(-1, 1)\n",
    "\n",
    "preprocess_pipe = create_poly_feature_pipe(degree, normalize=True)\n",
    "preprocess_pipe.fit(data2_train_X)\n",
    "\n",
    "model = Lasso()\n",
    "train_rmse, valid_rmse, weights = compute_validate_curve(model, preprocess_pipe, \n",
    "                                                data2_train_set, data2_valid_set, lambda_vec)\n",
    "plot_validate_curve(train_rmse, valid_rmse, lambda_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scikit-learn Lasso $\\lambda$ on weights effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot weights with lambda regularization\n",
    "weights_name = ['{:2d} th poly'.format(d) for d in np.arange(1, degree + 1)]\n",
    "weights = weights[:, 1:] # remove bias term\n",
    "plot_lambda_weights_effect(weights, lambda_vec, weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find lambda with lowest train and valid rmse, skip lambda = 0\n",
    "opt_lambda = lambda_vec[np.argmin(np.abs(train_rmse[1:] - valid_rmse[1:])) + 1]\n",
    "data2_train_X_tr = preprocess_pipe.transform(data2_train_X)\n",
    "model.alpha = opt_lambda\n",
    "model.fit(data2_train_X_tr, data2_train_y)\n",
    "\n",
    "print(\"optimal lambda {:2.6e}\".format(opt_lambda))\n",
    "plot_model_result(model, preprocess_pipe, data2_train_set, data2_valid_set)\n",
    "show_model_result(model, preprocess_pipe, data2_train_set, data2_valid_set, data2_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regularization with Tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ex2data3.csv')\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "split_train, split_valid, split_test = train_val_test_split(df, 0.6, 0.2, 0.2)\n",
    "\n",
    "data3_train_X, data3_train_y = split_train.iloc[:, 0:-1].values.reshape(-1, 8), \\\n",
    "                               split_train.iloc[:, -1].values.reshape(-1, 1) \n",
    "data3_valid_X, data3_valid_y = split_valid.iloc[:, 0:-1].values.reshape(-1, 8), \\\n",
    "                               split_valid.iloc[:, -1].values.reshape(-1, 1)\n",
    "data3_test_X ,  data3_test_y = split_test.iloc[:, 0:-1].values.reshape(-1, 8), \\\n",
    "                               split_test.iloc[:, -1].values.reshape(-1, 1)\n",
    "\n",
    "data3_train_set = (data3_train_X, data3_train_y)\n",
    "data3_valid_set = (data3_valid_X, data3_valid_y)\n",
    "data3_test_set  = (data3_test_X, data3_test_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the training data using regression model with ridge (L2 penalty) regularization with scikit-learn Ridge model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "degree = 1\n",
    "\n",
    "grid = np.linspace(-2, 5, 50)\n",
    "lambda_vec = np.array([0] + [np.power(10.0, p) for p in grid])\n",
    "#lambda_vec = np.r_[0, np.logspace(-2, 5, 20)].reshape(-1, 1)\n",
    "\n",
    "preprocess_pipe = create_poly_feature_pipe(degree, normalize=True)\n",
    "preprocess_pipe.fit(data3_train_X)\n",
    "\n",
    "model = Ridge()\n",
    "train_rmse, valid_rmse, weights = compute_validate_curve(model, preprocess_pipe, \n",
    "                                                data3_train_set, data3_valid_set, lambda_vec)\n",
    "plot_validate_curve(train_rmse, valid_rmse, lambda_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scikit-learn Ridge $\\lambda$ on weights effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot weights with lambda regularization\n",
    "weights_name = df.columns.values[0:-1]\n",
    "weights = weights[:, 1:] # remove bias term\n",
    "plot_lambda_weights_effect(weights, lambda_vec, weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find lambda with lowest train and valid rmse, skip lambda = 0\n",
    "opt_lambda = lambda_vec[np.argmin(np.abs(train_rmse[1:] - valid_rmse[1:])) + 1]\n",
    "data3_train_X_tr = preprocess_pipe.transform(data3_train_X)\n",
    "model.alpha = opt_lambda\n",
    "model.fit(data3_train_X_tr, data3_train_y)\n",
    "\n",
    "print(\"optimal lambda {:2.6e}\".format(opt_lambda))\n",
    "show_model_result(model, preprocess_pipe, data3_train_set, data3_valid_set, data3_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the training data using regression model with lasso (L1 penalty) regularization with scikit-learn Lasso model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "degree = 1\n",
    "\n",
    "grid = np.linspace(-2, 5, 50)\n",
    "lambda_vec = np.array([0] + [np.power(10.0, p) for p in grid])\n",
    "#lambda_vec = np.r_[0, np.logspace(-2, 5, 20)].reshape(-1, 1)\n",
    "\n",
    "preprocess_pipe = create_poly_feature_pipe(degree, normalize=True)\n",
    "preprocess_pipe.fit(data3_train_X)\n",
    "\n",
    "model = Lasso()\n",
    "train_rmse, valid_rmse, weights = compute_validate_curve(model, preprocess_pipe, \n",
    "                                                data3_train_set, data3_valid_set, lambda_vec)\n",
    "plot_validate_curve(train_rmse, valid_rmse, lambda_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso train and valid set error is very small, its difficult to viuslaize the difference on graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Log scale RMSE error')\n",
    "plt.semilogy(lambda_vec, train_rmse, color='g', label='Train')\n",
    "plt.semilogy(lambda_vec, valid_rmse, color='b', label='Valid')\n",
    "plt.xlabel('$lambda$')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scikit-learn Lasso $\\lambda$ on weights effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot weights with lambda regularization\n",
    "weights_name = df.columns.values[0:-1]\n",
    "weights = weights[:, 1:] # remove bias term\n",
    "plot_lambda_weights_effect(weights, lambda_vec, weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find lambda has lowest difference between train and valid rmse\n",
    "opt_lambda = lambda_vec[np.argmin(np.abs(train_rmse[1:] - valid_rmse[1:])) + 1]\n",
    "data3_train_X_tr = preprocess_pipe.transform(data3_train_X)\n",
    "model.alpha = opt_lambda\n",
    "model.fit(data3_train_X_tr, data3_train_y)\n",
    "\n",
    "print(\"optimal lambda {:2.6e}\".format(opt_lambda))\n",
    "show_model_result(model, preprocess_pipe, data3_train_set, data3_valid_set, data3_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the training data using regression model with ridge (L2 penalty) regularization using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFGradientDescentSolver(_BaseSolver, RegressorMixin):\n",
    "    def __init__(self, alpha=1.0, add_bias=True,\n",
    "                 learning_rate=1e-2, max_iter=None, tol=1e-8, regularizer='Ridge'):\n",
    "        self.alpha = alpha\n",
    "        self.add_bias = add_bias\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.regularizer = regularizer\n",
    "        \n",
    "    def fit(self, X, y, sess):\n",
    "        X, y = preprocess_data(X, y, self.add_bias)\n",
    "        \n",
    "        self.coef_, self.n_iter_, self.costs_ = \\\n",
    "            tf_gradient_descent(sess, X, y, \n",
    "                                lambda_=self.alpha, learning_rate=self.learning_rate,\n",
    "                                tol=self.tol, max_iter=self.max_iter, regularizer=self.regularizer)\n",
    "        \n",
    "\n",
    "def tf_gradient_descent(sess, X, y, lambda_=1, learning_rate=1e-2, tol=1e-8, max_iter=None, regularizer='Ridge'):\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    if max_iter is None:\n",
    "        max_iter = 1000\n",
    "    \n",
    "    mask = np.ones([n_features, 1])\n",
    "    mask[0] = 0\n",
    "    costs = np.zeros(max_iter)\n",
    "    \n",
    "    # TF variables\n",
    "    X_ = tf.placeholder(tf.float32, [None, n_features])\n",
    "    y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "    learning_rate_ = tf.constant(learning_rate)\n",
    "    reg_lambda_ = tf.constant(lambda_, dtype=tf.float32)\n",
    "    mask_ = tf.constant(mask, dtype=tf.float32)\n",
    "    w_ = tf.Variable(tf.zeros([n_features, 1]))\n",
    "    \n",
    "    # cost function term\n",
    "    diff_ = tf.matmul(X_, w_) - y\n",
    "    obj_cost_ = tf.reduce_mean(tf.pow(diff_, 2)) / 2\n",
    "    obj_grad_ = tf.matmul(tf.transpose(X_), diff_) / tf.cast(tf.shape(X_)[0], tf.float32)\n",
    "    \n",
    "    # regularization term\n",
    "    if regularizer == \"Ridge\":\n",
    "        reg_cost_ = tf.reduce_sum(tf.pow(mask_ * w_, 2)) / (2 * tf.cast(tf.shape(X_)[0], tf.float32))\n",
    "        reg_grad_ = (mask_ * w_) / tf.cast(tf.shape(X_)[0], tf.float32)\n",
    "        cost_ = obj_cost_ + reg_lambda_ * reg_cost_\n",
    "        grad_ = obj_grad_ + reg_lambda_ * reg_grad_\n",
    "        \n",
    "    elif regularizer == \"Lasso\":\n",
    "        # FIXME: Lasso soft_threshold implementation is wrong\n",
    "        #        Using tf.gradient to replace compute l1 gradient\n",
    "        reg_cost_ = tf.reduce_sum(tf.abs(mask_ * w_)) / (2 * tf.cast(tf.shape(X_)[0], tf.float32))\n",
    "        #reg_grad_ = w_ / (2 * tf.cast(tf.shape(X_)[0], tf.float32))\n",
    "        cost_ = obj_cost_ + reg_lambda_ * reg_cost_\n",
    "        #grad_ = obj_grad_ + reg_lambda_ * reg_grad_\n",
    "        grad_ = tf.gradients(cost_, xs=w_)\n",
    "        \n",
    "    # gradient update step\n",
    "    train_op = tf.assign(w_, tf.reshape(w_ - learning_rate_ * grad_, [n_features, 1]))\n",
    "    \n",
    "    feed_dict = {X_: X, y_: y}\n",
    "    \n",
    "    sess.run(tf.assign(w_, tf.zeros([n_features, 1])))\n",
    "    costs[0] = sess.run(cost_, feed_dict)\n",
    "    for n_iter in np.arange(1, max_iter):\n",
    "        _ = sess.run(train_op, feed_dict)\n",
    "        costs[n_iter] = sess.run(cost_, feed_dict)\n",
    "        \n",
    "        #if n_iter % 50 == 0:\n",
    "        #    costs[n_iter] = sess.run(cost_, feed_dict)\n",
    "        #    print('costs[%4d]: %.8f' % (n_iter, costs[n_iter]))\n",
    "        if np.abs(costs[n_iter - 1] - costs[n_iter]) < tol:\n",
    "            costs = costs[:n_iter]\n",
    "            break\n",
    "    w = sess.run(w_)\n",
    "\n",
    "    return w, n_iter, costs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 1\n",
    "\n",
    "grid = np.linspace(-2, 5, 50)\n",
    "lambda_vec = np.array([0] + [np.power(10.0, p) for p in grid])\n",
    "#lambda_vec = np.r_[0, np.logspace(-2, 5, 20)].reshape(-1, 1)\n",
    "\n",
    "train_rmse = np.zeros(len(lambda_vec))\n",
    "valid_rmse = np.zeros(len(lambda_vec))\n",
    "\n",
    "preprocess_pipe = create_poly_feature_pipe(degree, normalize=True)\n",
    "data3_train_X_tr = preprocess_pipe.fit_transform(data3_train_X)\n",
    "data3_valid_X_tr = preprocess_pipe.transform(data3_valid_X)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "        \n",
    "    weights = []\n",
    "    for idx, lambda_ in enumerate(lambda_vec):\n",
    "\n",
    "        model = TFGradientDescentSolver(lambda_, learning_rate=0.1, max_iter=2000)\n",
    "        model.fit(data3_train_X_tr, data3_train_y, sess)\n",
    "        weights.append(extract_model_weights(model))\n",
    "        \n",
    "        train_rmse[idx] = compute_rmse(data3_train_y, model.predict(data3_train_X_tr))\n",
    "        valid_rmse[idx] = compute_rmse(data3_valid_y, model.predict(data3_valid_X_tr))\n",
    "    \n",
    "weights = np.squeeze(np.array(weights))\n",
    "    \n",
    "plot_validate_curve(train_rmse, valid_rmse, lambda_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow Gradient Descent with ridge (L2 penalty) regularization $\\lambda$ on weights effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot weights with lambda regularization\n",
    "weights_name = df.columns.values[:-1]\n",
    "weights = weights[:, 1:] # remove bias term\n",
    "plot_lambda_weights_effect(weights, lambda_vec, weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find lambda has lowest difference between train and valid rmse\n",
    "opt_lambda = lambda_vec[np.argmin(np.abs(train_rmse[1:] - valid_rmse[1:])) + 1]\n",
    "data3_train_X_tr = preprocess_pipe.transform(data3_train_X)\n",
    "model.alpha = opt_lambda\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model.fit(data3_train_X_tr, data3_train_y, sess)\n",
    "\n",
    "print(\"optimal lambda {:2.6e}\".format(opt_lambda))\n",
    "show_model_result(model, preprocess_pipe, data3_train_set, data3_valid_set, data3_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the training data using regression model with lasso (L1 penalty) regularization using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 1\n",
    "\n",
    "grid = np.linspace(-2, 5, 50)\n",
    "lambda_vec = np.array([0] + [np.power(10.0, p) for p in grid])\n",
    "#lambda_vec = np.r_[0, np.logspace(-2, 5, 20)].reshape(-1, 1)\n",
    "\n",
    "train_rmse = np.zeros(len(lambda_vec))\n",
    "valid_rmse = np.zeros(len(lambda_vec))\n",
    "\n",
    "preprocess_pipe = create_poly_feature_pipe(degree, normalize=True)\n",
    "data3_train_X_tr = preprocess_pipe.fit_transform(data3_train_X)\n",
    "data3_valid_X_tr = preprocess_pipe.transform(data3_valid_X)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    weights = []\n",
    "    for idx, lambda_ in enumerate(lambda_vec):\n",
    "\n",
    "        model = TFGradientDescentSolver(lambda_, regularizer='Lasso', max_iter=3000)\n",
    "        model.fit(data3_train_X_tr, data3_train_y, sess)\n",
    "        weights.append(extract_model_weights(model))\n",
    "    \n",
    "        train_rmse[idx] = compute_rmse(data3_train_y, model.predict(data3_train_X_tr))\n",
    "        valid_rmse[idx] = compute_rmse(data3_valid_y, model.predict(data3_valid_X_tr))\n",
    "\n",
    "weights = np.squeeze(np.array(weights))    \n",
    "    \n",
    "plot_validate_curve(train_rmse, valid_rmse, lambda_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow Gradient Descent with lasso (L1 penalty) regularization $\\lambda$ on weights effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot weights with lambda regularization\n",
    "weights_name = df.columns.values[:-1]\n",
    "weights = weights[:, 1:] # remove bias term\n",
    "plot_lambda_weights_effect(weights, lambda_vec, weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find lambda has lowest difference between train and valid rmse\n",
    "opt_lambda = lambda_vec[np.argmin(np.abs(train_rmse[1:] - valid_rmse[1:])) + 1]\n",
    "data3_train_X_tr = preprocess_pipe.transform(data3_train_X)\n",
    "model.alpha = opt_lambda\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model.fit(data3_train_X_tr, data3_train_y, sess)\n",
    "\n",
    "print(\"optimal lambda {:2.6e}\".format(opt_lambda))\n",
    "show_model_result(model, preprocess_pipe, data3_train_set, data3_valid_set, data3_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
