{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "np.random.seed(1)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Function define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(x, n_class):\n",
    "    \"\"\"\n",
    "    One Hot encoding\n",
    "    \n",
    "    Inputs:\n",
    "    - x: N smaple vector\n",
    "    - n_class: Number of class\n",
    "    \n",
    "    Returns:\n",
    "    - en_1hot: Encoding matrix shape of (n_smaple, n_class)\n",
    "    \"\"\"\n",
    "    \n",
    "    en_1hot = np.zeros([len(x), n_class])\n",
    "    \n",
    "    for idx, cat in enumerate(x):\n",
    "        en_1hot[idx, cat] = 1\n",
    "\n",
    "    return en_1hot\n",
    "\n",
    "def next_batch(X, y, batch_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    Get next batch data\n",
    "    \n",
    "    Inputs: \n",
    "    - X: input data\n",
    "    - y: input data label\n",
    "    - batch_size: s\n",
    "    \n",
    "    Outputs tuple of batch data \n",
    "    - X_batch: batch sampled X \n",
    "    - y_batch: batch sampled y\n",
    "    \"\"\"\n",
    "    \n",
    "    n_sample = X.shape[0]\n",
    "    n_batch = n_sample // batch_size\n",
    "    n_batch = n_batch + 1 if (n_sample % n_batch) != 0 else n_batch \n",
    "    idx = np.array(range(n_sample))\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "        \n",
    "    for b_idx in range(n_batch):\n",
    "        start, end = b_idx * batch_size, (b_idx + 1) * batch_size\n",
    "        if end >= n_sample:\n",
    "            sample_idx = idx[start:]\n",
    "        else:\n",
    "            sample_idx = idx[start:end] \n",
    "        \n",
    "        X_batch, y_batch = X[sample_idx, :], y[sample_idx, :]\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \n",
    "    x_reshape = x.reshape(x.shape[0], -1)\n",
    "    # trick to avoid numerical unstable\n",
    "    probs = np.exp(x_reshape - np.max(x_reshape, axis=-1, keepdims=True))\n",
    "    probs /= np.sum(probs, axis=-1, keepdims=True)\n",
    "    \n",
    "    return probs\n",
    "    \n",
    "\n",
    "def xavier_initializer(n_inputs, n_outputs, shape):\n",
    "    init_range = np.sqrt(6.0 / (n_inputs + n_outputs))\n",
    "    return  np.random.uniform(-init_range, init_range, shape)\n",
    "\n",
    "\n",
    "def im2col_idx(x_shape, KH, KW, pad, stride):\n",
    "    \"\"\"\n",
    "    Get im2col index based on kerenl size\n",
    "    \n",
    "    Inputs:\n",
    "    - x_shape: Shape of input in (NH, NW, ND, NB) order\n",
    "    - KH: Kernel height\n",
    "    - KW: Kernel width\n",
    "    - pad: Padding step\n",
    "    - stride: Stride step\n",
    "    \n",
    "    Returns a tuple of idx\n",
    "    - r_idx: Kernel slide row idx\n",
    "    - c_idx: Kernel slide col idx\n",
    "    - d_idx: Kernel slide depth idx\n",
    "    \"\"\"\n",
    "    \n",
    "    #(img_h, img_w, img_c, n_batch)\n",
    "    NH, NW, ND, NB = x_shape\n",
    "    OH = (NH + 2 * pad - KH) // stride + 1\n",
    "    OW = (NW + 2 * pad - KW) // stride + 1\n",
    "    \n",
    "    # each block row had KH cols \n",
    "    # compute row idx \n",
    "    r_idx = np.repeat(np.arange(KH), KW)\n",
    "    r_idx = np.tile(r_idx, ND)\n",
    "    # compute row block shfit\n",
    "    r_shift = stride * np.repeat(np.arange(OH), OW)\n",
    "    r_idx = r_idx.reshape(-1, 1) + r_shift.reshape(1, -1)\n",
    "    \n",
    "    # compute col idx\n",
    "    c_idx = np.tile(np.arange(KW), KH * ND)\n",
    "    # compute col block shift\n",
    "    c_shift = stride * np.tile(np.arange(OW), OH)\n",
    "    c_idx = c_idx.reshape(-1, 1) + c_shift.reshape(1, -1)\n",
    "    \n",
    "    d_idx = np.repeat(np.arange(ND), KH * KW).reshape(-1, 1)\n",
    "    \n",
    "    return (r_idx, c_idx, d_idx)\n",
    "    \n",
    "\n",
    "def im2col(x, KH, KW, pad, stride):\n",
    "    \"\"\"\n",
    "    Convert input x along kernel size\n",
    "    \n",
    "    Extract x into kernel patch cols \n",
    "    \n",
    "    Inputs:\n",
    "    - x_shape: Shape of input in (NH, NW, ND, NB) order\n",
    "    - KH: Kernel height\n",
    "    - KW: Kernel width\n",
    "    - pad: Pad step\n",
    "    - stride: Stride step\n",
    "    \n",
    "    Returns:\n",
    "    - cols: Each cols contain KH * KW * ND elements\n",
    "    \"\"\"\n",
    "    \n",
    "    NH, NW, ND, NB = x.shape\n",
    "    x_pad = np.pad(x, \n",
    "                   # only padding on height and weight\n",
    "                   ((pad, pad),    # height\n",
    "                    (pad, pad),    # width \n",
    "                    (0, 0),        # channel\n",
    "                    (0, 0)),       # batch\n",
    "                   'constant', constant_values=0)\n",
    "    \n",
    "    # get patch index\n",
    "    r, c, d = im2col_idx(x.shape, KH, KW, pad, stride)\n",
    "    # extract correspond patch to cols\n",
    "    cols = x_pad[r, c, d, :]\n",
    "    cols = cols.reshape(KH * KW * ND, -1)\n",
    "    \n",
    "    return cols\n",
    "\n",
    "\n",
    "def col2im(cols, x_shape, KH, KW, pad, stride):\n",
    "    \"\"\"Put col kerenel patch back to x\n",
    "    \n",
    "    Inputs:\n",
    "    - x_shape: Shape of input in (NH, NW, ND, NB) order\n",
    "    - KH: Kernel height\n",
    "    - KW: Kernel width\n",
    "    - pad: Pad step\n",
    "    - stride: Stride step\n",
    "    \n",
    "    Returns: \n",
    "    - x_pad: a pad x matrix accroding cols patch\n",
    "    \"\"\"\n",
    "    \n",
    "    NH, NW, ND, NB = x_shape\n",
    "    OH, OW = NH + 2 * pad, NW + 2 * pad\n",
    "    \n",
    "    x_pad = np.zeros((OH, OW, ND, NB), dtype=cols.dtype)\n",
    "    r, c, d = im2col_idx(x_shape, KH, KW, pad, stride)\n",
    "\n",
    "    # add correspond patch back to location\n",
    "    cols_reshape = cols.reshape(KH * KW * ND, -1, NB)    \n",
    "    np.add.at(x_pad, (r, c, d, slice(None)), cols_reshape)\n",
    "    if pad == 0:\n",
    "        return x_pad\n",
    "    return x_pad[pad:-pad, pad:-pad, :, :]\n",
    "\n",
    "\n",
    "def get_output_size(input_size, ksize, n_filters, stride, pad):\n",
    "    \"\"\"\n",
    "    Get output size of layer\n",
    "    \n",
    "    Inputs:\n",
    "    - input_size: shape of input data\n",
    "    - ksize: kernel size\n",
    "    - n_filters: number of output channel\n",
    "    - stride: Stride steps\n",
    "    - pad: Pad steps\n",
    "    \n",
    "    Returns:\n",
    "    - output_size: output size\n",
    "    \"\"\"\n",
    "    \n",
    "    NH, NW, ND = input_size\n",
    "    KH, KW = ksize\n",
    "    OH = (NH + 2 * pad - KH) // stride + 1\n",
    "    OW = (NW + 2 * pad - KW) // stride + 1\n",
    "\n",
    "    output_size = (OH, OW, n_filters)\n",
    "    return output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foward and backward operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_foward(x, w, b):\n",
    "    \"\"\"\n",
    "    Fully connected foward\n",
    "    \n",
    "    Inputs:\n",
    "    - x: input data of shape (N, d_1, ... d_k)\n",
    "    - w: weights of shape (D, M)\n",
    "    - b: bias of shape (M, )\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    - out: output of shape (N, M)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    \n",
    "    NB = x.shape[0]\n",
    "    \n",
    "    x_reshape = x.reshape(NB, -1)\n",
    "    out = np.dot(x_reshape, w) + b\n",
    "\n",
    "    cache = (x, w, b)\n",
    "    \n",
    "    return out, cache\n",
    "\n",
    "def fc_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Fully connected backward\n",
    "    \n",
    "    Inputs:\n",
    "    - dout: Gradient of output, of shape (N, M)\n",
    "    - cache: Tuple of:\n",
    "      - x: Input data, of shape (N, d_1, ... d_k)\n",
    "      - w: Weights, of shape (D, M)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient of x, of shape (N, d1, ..., d_k)\n",
    "    - dw: Gradient of w, of shape (D, M)\n",
    "    - db: Gradient of b, of shape (M,)\n",
    "    \"\"\"\n",
    "    \n",
    "    x, w, b = cache\n",
    "    NB = x.shape[0]\n",
    "    \n",
    "    x_reshape = x.reshape(NB, -1)\n",
    "    \n",
    "    dx = np.dot(dout, w.T)\n",
    "    dx = dx.reshape(x.shape)\n",
    "    dw = np.dot(x_reshape.T, dout)\n",
    "    db = np.sum(dout, axis=0)\n",
    "    \n",
    "    return dx, dw, db\n",
    "\n",
    "def conv_foward_navie(x, w, b, params):\n",
    "    pass\n",
    "\n",
    "def conv_backword_navie(dout, cache):\n",
    "    pass\n",
    "    \n",
    "def maxpool_foward_navie(x, param):\n",
    "    pass\n",
    "\n",
    "def maxpool_backward_navie(dout, cache):\n",
    "    pass\n",
    "\n",
    "def conv_foward(x, w, b, params):\n",
    "    \"\"\"\n",
    "    Fast conv layer foward implementation using im2col\n",
    "    \n",
    "    Inputs:\n",
    "    - x: input data\n",
    "    - w: weights\n",
    "    - b: bias\n",
    "    - params: conv config params\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    - out: conv output\n",
    "    - cache: (x, w, b, params, x_cols)\n",
    "    \"\"\"\n",
    "    \n",
    "    NB, NH, NW, ND = x.shape\n",
    "    KH, KW, ND, KF = w.shape\n",
    "    stride = params['stride']\n",
    "    pad = params['pad']\n",
    "    \n",
    "    OH = (NH + 2 * pad - KH) // stride + 1\n",
    "    OW = (NW + 2 * pad - KW) // stride + 1\n",
    "    \n",
    "    # switch n_batch to last dim\n",
    "    x = x.transpose(1, 2, 3, 0)\n",
    "    x_cols = im2col(x, KH, KW, pad, stride)\n",
    "    w_cols = w.transpose(3, 0, 1, 2).reshape(KF, -1)\n",
    "\n",
    "    out = w_cols.dot(x_cols) + b.reshape(-1, 1)\n",
    "    out = out.reshape(KF, OH, OW, NB)\n",
    "    # swith n_batch to first dim\n",
    "    x = x.transpose(3, 0, 1, 2)\n",
    "    out = out.transpose(3, 1, 2, 0)\n",
    "    \n",
    "    cache = (x, w, b, params, x_cols)\n",
    "    \n",
    "    return out, cache\n",
    "    \n",
    "def conv_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Fast conv layer backward implementation using im2col\n",
    "    \n",
    "    Inputs:\n",
    "    - dout: Gradient of output\n",
    "    - cache: Tuple of:\n",
    "      - x: Input data\n",
    "      - w: Weights\n",
    "      - b: Bias\n",
    "      - params: Conv config params\n",
    "      - x_cols: Input data along extract along kernel size path matrix\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient of x\n",
    "    - dw: Gradient of w\n",
    "    - db: Gradient of b\n",
    "    \"\"\"\n",
    "\n",
    "    x, w, b, params, x_cols = cache\n",
    "    NB, NH, NW, ND = x.shape\n",
    "    KH, KW, ND, KF = w.shape\n",
    "    stride = params['stride']\n",
    "    pad = params['pad']\n",
    "    \n",
    "    db = np.sum(dout, axis=(0, 1, 2))\n",
    "    w_cols = w.transpose(3, 0, 1, 2).reshape(KF, -1)\n",
    "    # switch [NB, NH, NW, NC] -> [NC, NH, NW, NB]\n",
    "    dout = dout.transpose(3, 1, 2, 0)\n",
    "    dout = dout.reshape(KF, -1)\n",
    "    w_cols = w.transpose(3, 0, 1, 2).reshape(KF, -1)\n",
    "    dx_cols = w_cols.T.dot(dout)\n",
    "    dx = col2im(dx_cols, (NH, NW, ND, NB), KH, KW, pad, stride)\n",
    "    \n",
    "    dw_cols = dout.dot(x_cols.T)\n",
    "    dw = dw_cols.reshape(KF, KH, KW, ND)\n",
    "    dw = dw.transpose(1, 2, 3, 0)\n",
    "    \n",
    "    return dx, dw, db\n",
    "\n",
    "\n",
    "def maxpool_foward(x, params):\n",
    "    \"\"\"\n",
    "    Fast max pool layer foward implementation using im2col\n",
    "    \n",
    "    Inputs:\n",
    "    - x: Input data\n",
    "    - params: Maxpool config params\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    - out: Maxpool output\n",
    "    - cache: (x, params, x_cols, x_cols_argmax)\n",
    "    \"\"\"\n",
    "    \n",
    "    NB, NH, NW, ND = x.shape\n",
    "    stride = params['stride']\n",
    "    pad = params['pad']\n",
    "    KH, KW = params['ksize']\n",
    "    \n",
    "    OH = (NH + 2 * pad - KH) // stride + 1\n",
    "    OW = (NW + 2 * pad - KW) // stride + 1\n",
    "    \n",
    "    # take batch sample as channels\n",
    "    x_pool = x.transpose(1, 2, 3, 0)\n",
    "    x_pool = x_pool.reshape(NH, NW, 1, -1)\n",
    "    x_cols = im2col(x_pool, KH, KW, pad, stride)\n",
    "    # perform max pool\n",
    "    # get max value index\n",
    "    x_cols_argmax = np.argmax(x_cols, axis=0)\n",
    "\n",
    "    # x_col: [block_size, n_blocks]\n",
    "    # each block get its max value\n",
    "    x_cols_max = x_cols[x_cols_argmax, np.arange(x_cols.shape[1])]\n",
    "    out = x_cols_max.reshape(OH, OW, ND, NB)\n",
    "    out = out.transpose(3, 0, 1, 2)\n",
    "    \n",
    "    cache = (x, params, x_cols, x_cols_argmax)\n",
    "    \n",
    "    return out, cache\n",
    "\n",
    "def maxpool_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Fast conv layer backward implementation using im2col\n",
    "    \n",
    "    Inputs:\n",
    "    - dout: Gradient of output\n",
    "    - cache: Tuple of:\n",
    "      - x: Input data\n",
    "      - params: Conv config params\n",
    "      - x_cols: Input data along extract along kernel size path matrix\n",
    "      - x_cols_argmax: Max pool kernel path max idx\n",
    "      \n",
    "    Returns:\n",
    "    - dx: Gradient of x\n",
    "    \"\"\"\n",
    "    \n",
    "    x, params, x_cols, x_cols_argmax = cache\n",
    "    NB, NH, NW, ND = x.shape\n",
    "    stride = params['stride']\n",
    "    pad = params['pad']\n",
    "    KH, KW = params['ksize']\n",
    "    \n",
    "    dout_reshape = dout.transpose(1, 2, 3, 0).ravel()\n",
    "    dx_cols = np.zeros_like(x_cols)\n",
    "    dx_cols[x_cols_argmax, np.arange(dx_cols.shape[1])] = dout_reshape\n",
    "    dx = col2im(dx_cols, (NH, NW, 1, ND * NB), KH, KW, pad, stride)\n",
    "    \n",
    "    dx = dx.reshape(NH, NW, ND, NB)\n",
    "    dx = dx.transpose(3, 0, 1, 2)\n",
    "    \n",
    "    return dx\n",
    "\n",
    "\n",
    "def sigmoid_foward(x):\n",
    "    \n",
    "    cache = x\n",
    "    \n",
    "    out = 1. / (1. + np.exp(-x))\n",
    "    # trick to avoid numerical unstable\n",
    "    #out = np.exp(x) / (np.exp(x) + np.exp(0))\n",
    "    \n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def sigmoid_backward(dout, cache):\n",
    "    \n",
    "    cache = x\n",
    "    dx = dout * (1. - dout)\n",
    "    \n",
    "    return dx\n",
    "\n",
    "\n",
    "def relu_forward(x):\n",
    "    \n",
    "    cache = x\n",
    "    out = np.maximum(x, 0)\n",
    "    \n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def relu_backward(dout, cache):\n",
    "    \n",
    "    x = cache\n",
    "    dx = np.where(x > 0, dout, 0)\n",
    "    \n",
    "    return dx\n",
    "\n",
    "\n",
    "def logistic_loss(x, y):\n",
    "    \"\"\"\n",
    "    Compute logistic loss and gradient\n",
    "    \n",
    "    Inputs:\n",
    "    - x: Conv network raw reslut after sigmoid prob\n",
    "    - y: True label one hot encoding matrix\n",
    "    \n",
    "    Outpus:\n",
    "    - loss: Logistic loss\n",
    "    - dx: Gradient of x\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    loss = y * np.log(x) + (1 - y) * np.log(1 - x)\n",
    "    loss = -np.mean(np.sum(loss, axis=-1, keepdims=True))\n",
    "    \n",
    "    dx = (x - y) / x.shape[0]\n",
    "    \n",
    "    return loss, dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model foward and backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_foward(x, layer, layer_type):\n",
    "    \"\"\"\n",
    "    Layer foward\n",
    "    \n",
    "    Wrap different type of foward operation\n",
    "    \n",
    "    Inputs:\n",
    "    - x: input data\n",
    "    - layer: Layer data structure in dictionary\n",
    "    - layer_type: Layer type\n",
    "    \n",
    "    Outputs:\n",
    "    - out: Layer fowrard output \n",
    "    - cache: Layer forward cache\n",
    "    \"\"\"\n",
    "      \n",
    "    out, cache = None, None\n",
    "    if layer_type == 'conv':\n",
    "        \n",
    "        params = layer['conv_params']\n",
    "        w, b = layer['w'], layer['b']\n",
    "        out, cache = conv_foward(x, w, b, params)\n",
    "    \n",
    "    elif layer_type == 'pool':\n",
    "    \n",
    "        params = layer['pool_params']\n",
    "        out, cache = maxpool_foward(x, params)\n",
    "        \n",
    "    elif layer_type == 'fc':\n",
    "        \n",
    "        w, b = layer['w'], layer['b']\n",
    "        out, cache = fc_foward(x, w, b)\n",
    "    \n",
    "    elif layer_type == 'relu':\n",
    "        \n",
    "        out, cache = relu_forward(x)\n",
    "        \n",
    "    elif layer_type == 'sigmoid':\n",
    "        \n",
    "        out, cache = sigmoid_foward(x)\n",
    "        \n",
    "    return out, cache\n",
    "\n",
    "def layer_backward(dout, cache, layer, layer_type):\n",
    "    \"\"\"\n",
    "    Layer backward\n",
    "    \n",
    "    Wrap different type backward operations\n",
    "    \n",
    "    Inputs:\n",
    "    - dout: Gradient of output\n",
    "    - cache: Foward reslut cache\n",
    "    - layer: Layer data structure in dictionary\n",
    "    - layer_type: Layer type\n",
    "    \n",
    "    \"\"\"\n",
    "      \n",
    "    dx = None\n",
    "    if layer_type == 'conv':\n",
    "        \n",
    "        dx, dw, db = conv_backward(dout, cache)\n",
    "        #layer['grad'] = {'dw': dw, 'db': db}\n",
    "        grad = layer['grad']\n",
    "        grad['dw'][:] = dw\n",
    "        grad['db'][:] = db\n",
    "        \n",
    "        \n",
    "    elif layer_type == 'pool':\n",
    "\n",
    "        dx = maxpool_backward(dout, cache)\n",
    "        \n",
    "    elif layer_type == 'fc':\n",
    "        \n",
    "        dx, dw, db = fc_backward(dout, cache)\n",
    "        #layer['grad'] = {'dw': dw, 'db': db}\n",
    "        grad = layer['grad']\n",
    "        grad['dw'][:] = dw\n",
    "        grad['db'][:] = db\n",
    "    \n",
    "    elif layer_type == 'relu':\n",
    "        \n",
    "        dx = relu_backward(dout, cache)\n",
    "        \n",
    "    elif layer_type == 'sigmoid':\n",
    "        \n",
    "        dx = sigmoid_backward(dout, cache)\n",
    "        \n",
    "    return dx\n",
    "\n",
    "def model_predict(model, x):\n",
    "    \n",
    "    out, _ = model_foward(model, x)\n",
    "    prob = softmax(out)\n",
    "    pred = np.argmax(prob, axis=1)\n",
    "    \n",
    "    return pred\n",
    "    \n",
    "\n",
    "def model_foward(model, x):\n",
    "    \"\"\"\n",
    "    Model forward\n",
    "    \n",
    "    Inputs:\n",
    "    - model: CNN Model\n",
    "    - x : input data\n",
    "    \n",
    "    Outputs tuple of:\n",
    "    - out: Model foward output\n",
    "    - model_cache: Model intermedia result cache\n",
    "    \"\"\"\n",
    "    \n",
    "    model_cache = []\n",
    "    \n",
    "    for layer in model:\n",
    "        layer_type = layer['name']\n",
    "        sub_layers = layer_type.split('_')\n",
    "        \n",
    "        layer_cache = []\n",
    "        for sub_layer in sub_layers:\n",
    "            out, cache = layer_foward(x, layer, sub_layer)\n",
    "            x = out\n",
    "            layer_cache.append(cache)\n",
    "        model_cache.append(layer_cache)\n",
    "    \n",
    "    return out, model_cache\n",
    "\n",
    "def model_backward(model, dout, model_cache):\n",
    "    \"\"\"\n",
    "    Model backward\n",
    "    \n",
    "    Inputs:\n",
    "    - model: CNN Model\n",
    "    - dout: Gradient of output\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    for layer, layer_cache in zip(reversed(model), reversed(model_cache)):\n",
    "        layer_type = layer['name']\n",
    "        sub_layers = layer_type.split('_')\n",
    "                                  \n",
    "        for sub_layer, cache in zip(reversed(sub_layers), reversed(layer_cache)):\n",
    "            dx = layer_backward(dout, cache, layer, sub_layer)\n",
    "            dout = dx\n",
    "            \n",
    "    return dx\n",
    "\n",
    "def model_update(model, learning_rate):\n",
    "    \n",
    "    for layer in model:\n",
    "        grad = layer['grad']\n",
    "        if grad is not None:\n",
    "            # TODO: l2 reg\n",
    "            w, b = layer['w'], layer['b']\n",
    "            \n",
    "            w[:] = w - learning_rate * (grad['dw'])\n",
    "            b[:] = b - learning_rate * (grad['db'])\n",
    "            \n",
    "\n",
    "def sgd(model, X, y, epochs=100, learning_rate=1e-2, batch_size=32, verbose=False):\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent\n",
    "    \n",
    "    Inputs:\n",
    "    - model:\n",
    "    - X:\n",
    "    - y:\n",
    "    - epochs:\n",
    "    - learning_rate:\n",
    "    - batch_size:\n",
    "    \"\"\"\n",
    "    \n",
    "    history_loss = []\n",
    "    for e in range(epochs):\n",
    "    \n",
    "        batch_loss = []\n",
    "        for X_batch, y_batch in next_batch(X, y, batch_size, shuffle=True):\n",
    "            \n",
    "            out, cache = model_foward(model, X_batch)\n",
    "\n",
    "            # Model does not have activation in last layer\n",
    "            # To use logistic loss we need to do sigmoid to covnert to prob\n",
    "            # And can compute logistic loss y_hat - y\n",
    "            prob, _ = sigmoid_foward(out)\n",
    "            loss, dx = logistic_loss(prob, y_batch) \n",
    "            \n",
    "            #print('loss: {:4.2f}'.format(loss))\n",
    "            #print('true:', np.argmax(y_batch[:8], axis=1))\n",
    "            #print('pred:', np.argmax(prob[:8], axis=1))\n",
    "            \n",
    "            # TODO: compute l2 reg here\n",
    "            \n",
    "            # TODO: Do we need return grad to support l2 reg ? \n",
    "            model_backward(model, dx, cache)\n",
    "        \n",
    "            model_update(model, learning_rate)\n",
    "            \n",
    "            batch_loss.append(loss)\n",
    "            \n",
    "        # compute avg loss\n",
    "        avg_loss = np.mean(batch_loss)\n",
    "        history_loss.append(avg_loss)\n",
    "        \n",
    "        if verbose:\n",
    "            print('[{:3d}|{:3d}] loss: {:4.4f}'.format(e + 1, epochs, avg_loss))\n",
    "        \n",
    "    return model, history_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_norm(X, mu=None, sigma=None):\n",
    "    \"\"\"\n",
    "    Standard normalize input data X with mu and sigma.\n",
    "    If mu and sigma not given compute from input data X\n",
    "    \n",
    "    Inputs:\n",
    "    - X: Input data of shape (N, H, W, C)\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    - X_sc: Standard normalize of X \n",
    "    - mu: Mean of x of shape\n",
    "    - sigma: standard deviation of x of shape\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    mu = np.mean(X, axis=0, keepdims=True)\n",
    "    X_sc = X - mu\n",
    "    sigma = np.std(X, axis=0, keepdims=True)\n",
    "    X_sc = X_sc / sigma\n",
    "    \n",
    "    return X_sc, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('ex5_train_x.npy')\n",
    "y = np.load('ex5_train_y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_en = one_hot_encode(y, 6)\n",
    "X_sc, mu, sigma = std_norm(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Initialize parameters (Weights, bias for each layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer, input_size, layer_type):\n",
    "    \"\"\"\n",
    "    Initialize layer weights and its output size\n",
    "    \n",
    "    Inputs:\n",
    "    - layer: Layer data structure in dictionary\n",
    "    - input_size: Input size of layer\n",
    "    - layer type: Layer type\n",
    "    \n",
    "    Returns:\n",
    "    - output_size: Layer output shapes\n",
    "    \"\"\"\n",
    "    \n",
    "    output_size = None\n",
    "    # only conv and fc layer need weights initialize\n",
    "    if layer_type == 'conv':  \n",
    "        \n",
    "        NH, NW, ND = input_size\n",
    "        params = layer['conv_params']\n",
    "        stride, pad = params['stride'], params['pad']\n",
    "        KH, KW = params['ksize']\n",
    "        KF = params['n_filters']\n",
    "        \n",
    "        output_size = \\\n",
    "            get_output_size(input_size, (KH, KW), KF, stride, pad)\n",
    "        \n",
    "        # weights initialize\n",
    "        n_inputs = KH * KW * ND\n",
    "        n_outputs = KH * KW * KF\n",
    "        \n",
    "        w = xavier_initializer(n_inputs, n_outputs, (KH, KW, ND, KF))\n",
    "        b = np.zeros((1, 1, 1, KF))\n",
    "        layer['w'], layer['b'] = w, b\n",
    "        \n",
    "        dw, db = np.zeros_like(w), np.zeros_like(b)\n",
    "        layer['grad'] = {'dw': dw, 'db': db}\n",
    "\n",
    "    elif layer_type == 'pool':\n",
    "        \n",
    "        NH, NW, ND = input_size\n",
    "        params = layer['pool_params']\n",
    "        stride, pad = params['stride'], params['pad']\n",
    "        KH, KW = params['ksize']\n",
    "\n",
    "        output_size = \\\n",
    "            get_output_size(input_size, (KH, KW), ND, stride, pad)\n",
    "                \n",
    "    elif layer_type == 'fc':\n",
    "        \n",
    "        params = layer['params']\n",
    "        n_inputs = np.prod(input_size)\n",
    "        n_outputs = params['n_outputs']\n",
    "        \n",
    "        output_size = n_outputs\n",
    "        \n",
    "        # weights initialize\n",
    "        w = xavier_initializer(n_inputs, n_outputs, (n_inputs, n_outputs))\n",
    "        b = np.zeros(n_outputs)\n",
    "        layer['w'], layer['b'] = w, b\n",
    "        \n",
    "        dw, db = np.zeros_like(w), np.zeros_like(b)\n",
    "        layer['grad'] = {'dw': dw, 'db': db}\n",
    "\n",
    "    # other layer\n",
    "    else:\n",
    "        output_size = input_size\n",
    "            \n",
    "    return output_size\n",
    "            \n",
    "def model_init(model, input_size):\n",
    "    \"\"\"\n",
    "    Initialize the cnn model\n",
    "    \n",
    "    Inputs:\n",
    "    - input_size: Input data size\n",
    "    \n",
    "    \"\"\"\n",
    "    for layer in model:\n",
    "        layer_type = layer['name']\n",
    "        for sub_layer in layer_type.split('_'):\n",
    "            input_size = layer_init(layer, input_size, sub_layer)\n",
    "        layer['output_size'] = input_size\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = [\n",
    "{\n",
    "    # Using '_' to define sandwish layer\n",
    "    'name': 'conv_relu_pool',\n",
    "    'conv_params': \n",
    "        {'ksize': (3, 3), 'stride': 1, 'pad': 1, 'n_filters': 8},\n",
    "    'pool_params':\n",
    "        {'ksize': (2, 2), 'stride': 2, 'pad': 0},\n",
    "    'grad': None,\n",
    "},\n",
    "{   \n",
    "    'name': 'conv_relu_pool',\n",
    "    'conv_params': \n",
    "        {'ksize': (3, 3), 'stride': 1, 'pad': 1, 'n_filters': 16},\n",
    "    'pool_params': \n",
    "        {'ksize': (2, 2), 'stride': 2, 'pad': 0},\n",
    "    'grad': None,\n",
    "},\n",
    "# {   \n",
    "#     'name': 'conv_relu_pool',\n",
    "#     'conv_params': \n",
    "#         {'ksize': (3, 3), 'stride': 1, 'pad': 1, 'n_filters': 32},\n",
    "#     'pool_params': \n",
    "#         {'ksize': (2, 2), 'stride': 2, 'pad': 0},\n",
    "#     'grad': None,\n",
    "# },\n",
    "# {   \n",
    "#     'name': 'conv_relu_pool',\n",
    "#     'conv_params': \n",
    "#         {'ksize': (3, 3), 'stride': 1, 'pad': 1, 'n_filters': 64},\n",
    "#     'pool_params': \n",
    "#         {'ksize': (2, 2), 'stride': 2, 'pad': 0},\n",
    "#     'grad': None,\n",
    "# },\n",
    "# {\n",
    "#     'name': 'conv_relu',\n",
    "#     'conv_params': \n",
    "#         {'ksize': (1, 1), 'stride': 1, 'pad': 0, 'n_filters': 8},\n",
    "#     'grad': None,\n",
    "# },\n",
    "{\n",
    "    'name': 'fc_relu',\n",
    "    'params': {'n_outputs': 128},\n",
    "    'grad': None\n",
    "},\n",
    "{    \n",
    "    # Don't add activation in last layer just output raw neuron output\n",
    "    # finally decide used softmax or logisitic\n",
    "    'name': 'fc',\n",
    "    'params': {'n_outputs': 6},\n",
    "    'grad': None\n",
    "},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (64, 64, 3)\n",
    "model = model_init(model, input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Optimization of Convolution Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_summary(model, input_size):\n",
    "    \n",
    "    msg_format = \\\n",
    "        '{type:6} | {sz:12s} | {ksize:12s} | {stride:6s} | {pad:6s}'\n",
    "            \n",
    "    msg_header = {\n",
    "            'type': 'Type', 'sz': 'Output Size', 'ksize': 'Kernel Size',\n",
    "            'stride': 'Stride', 'pad': 'Padding'\n",
    "    }\n",
    "    \n",
    "    def layer_summary(input_size, layer, layer_type):\n",
    "        \n",
    "        layer_info = {\n",
    "            'type': '', \n",
    "            'sz': '', 'chs': '', 'ksize': '',\n",
    "            'stride': '', 'pad': ''\n",
    "        }\n",
    "        \n",
    "        output_size = None\n",
    "        \n",
    "        if layer_type == 'conv':\n",
    "            \n",
    "            NH, NW, ND = input_size\n",
    "            params = layer['conv_params']\n",
    "            stride, pad = params['stride'], params['pad']\n",
    "            KH, KW = params['ksize']\n",
    "            KF = params['n_filters']\n",
    "            channel = KF\n",
    "            \n",
    "            output_size = \\\n",
    "                get_output_size(input_size, [KH, KW], KF, stride, pad)\n",
    "                \n",
    "            layer_info['sz'] = str(output_size)\n",
    "            layer_info['ksize'] = str(params['ksize'])\n",
    "            layer_info['stride'] = str(stride)\n",
    "            layer_info['pad'] = str(pad)\n",
    "            \n",
    "        elif layer_type == 'pool':\n",
    "            \n",
    "            NH, NW, ND = input_size\n",
    "            params = layer['pool_params']\n",
    "            stride, pad = params['stride'], params['pad']\n",
    "            KH, KW = params['ksize']\n",
    "            channel = ND\n",
    "            \n",
    "            output_size = \\\n",
    "                get_output_size(input_size, [KH, KW], ND, stride, pad)\n",
    "            \n",
    "            layer_info['sz'] = str(output_size)\n",
    "            layer_info['ksize'] = str((KH, KW))\n",
    "            layer_info['stride'] = str(stride)\n",
    "            layer_info['pad'] = str(pad)\n",
    "            \n",
    "        elif layer_type == 'fc':\n",
    "            \n",
    "            params = layer['params']\n",
    "            output_size = params['n_outputs']\n",
    "            \n",
    "            layer_info['sz'] = str(output_size)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            output_size = input_size\n",
    "            layer_info['sz'] = str(output_size)\n",
    "        \n",
    "        layer_info['type'] = layer_type\n",
    "        print(msg_format.format(**layer_info))\n",
    "        \n",
    "        return output_size\n",
    "    \n",
    "    print(msg_format.format(**msg_header))\n",
    "    print('-'*len(msg_format))\n",
    "    for layer in model:\n",
    "        layer_type = layer['name']\n",
    "        sub_layers = layer_type.split('_')\n",
    "        \n",
    "        for sub_layer in sub_layers:\n",
    "            output_size = layer_summary(input_size, layer, sub_layer)\n",
    "            input_size = output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type   | Output Size  | Kernel Size  | Stride | Padding\n",
      "----------------------------------------------------------\n",
      "conv   | (64, 64, 8)  | (3, 3)       | 1      | 1     \n",
      "relu   | (64, 64, 8)  |              |        |       \n",
      "pool   | (32, 32, 8)  | (2, 2)       | 2      | 0     \n",
      "conv   | (32, 32, 16) | (3, 3)       | 1      | 1     \n",
      "relu   | (32, 32, 16) |              |        |       \n",
      "pool   | (16, 16, 16) | (2, 2)       | 2      | 0     \n",
      "fc     | 128          |              |        |       \n",
      "relu   | 128          |              |        |       \n",
      "fc     | 6            |              |        |       \n"
     ]
    }
   ],
   "source": [
    "input_size = (64, 64, 3)\n",
    "model_summary(model, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warm start \n",
    "# model, loss_history = sgd(model, X_sc[:64, :], y_en[:64, :], \n",
    "#                           epochs=10, learning_rate=1e-2, batch_size=8, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1| 20] loss: 3.3474\n",
      "[  2| 20] loss: 2.0274\n",
      "[  3| 20] loss: 1.8821\n",
      "[  4| 20] loss: 1.4822\n",
      "[  5| 20] loss: 1.5801\n",
      "[  6| 20] loss: 1.0147\n",
      "[  7| 20] loss: 0.9734\n",
      "[  8| 20] loss: 0.8089\n",
      "[  9| 20] loss: 0.5287\n",
      "[ 10| 20] loss: 0.5753\n",
      "[ 11| 20] loss: 0.4980\n",
      "[ 12| 20] loss: 0.1836\n",
      "[ 13| 20] loss: 0.1858\n",
      "[ 14| 20] loss: 0.2215\n",
      "[ 15| 20] loss: 0.1620\n",
      "[ 16| 20] loss: 0.0764\n",
      "[ 17| 20] loss: 0.0541\n",
      "[ 18| 20] loss: 0.0279\n",
      "[ 19| 20] loss: 0.0163\n",
      "[ 20| 20] loss: 0.0112\n"
     ]
    }
   ],
   "source": [
    "model, loss_history = sgd(model, X_sc, y_en, \n",
    "                          epochs=20, learning_rate=1e-1, batch_size=64, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_predict(model, X_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.mean(pred == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('accuracy: ', 100.0)\n"
     ]
    }
   ],
   "source": [
    "print('accuracy: ', acc * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, 4, 0, 5, 0, 2, 0, 3, 5, 5, 2, 0, 4, 3, 1, 4, 4, 4, 1, 4,\n",
       "       0, 4, 4, 1, 0, 1, 2, 0, 1, 5, 2, 1, 1, 2, 3, 1, 4, 1, 4, 1, 1, 5,\n",
       "       5, 2, 2, 3, 0, 0, 1, 2, 1, 4, 4, 4, 0, 3, 4, 4, 0, 0, 5, 0, 4, 5,\n",
       "       5, 1, 5, 4, 1, 1, 0, 3, 2, 2, 2, 2, 0, 5, 2, 5, 1, 4, 2, 5, 4, 5,\n",
       "       1, 2, 3, 2, 4, 3, 3, 4, 4, 4, 1, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, 4, 0, 5, 0, 2, 0, 3, 5, 5, 2, 0, 4, 3, 1, 4, 4, 4, 1, 4,\n",
       "       0, 4, 4, 1, 0, 1, 2, 0, 1, 5, 2, 1, 1, 2, 3, 1, 4, 1, 4, 1, 1, 5,\n",
       "       5, 2, 2, 3, 0, 0, 1, 2, 1, 4, 4, 4, 0, 3, 4, 4, 0, 0, 5, 0, 4, 5,\n",
       "       5, 1, 5, 4, 1, 1, 0, 3, 2, 2, 2, 2, 0, 5, 2, 5, 1, 4, 2, 5, 4, 5,\n",
       "       1, 2, 3, 2, 4, 3, 3, 4, 4, 4, 1, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
