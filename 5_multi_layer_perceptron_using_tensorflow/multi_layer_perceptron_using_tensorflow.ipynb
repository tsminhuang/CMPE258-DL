{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMPE-258 Deep Learning \n",
    "\n",
    "Instructor: Dr. Taehee Jeong\n",
    "\n",
    "Student: Tsung-Min Huang\n",
    "\n",
    "ID: 012483146\n",
    "\n",
    "\n",
    "## Assignment 4 - Deep neuron network using TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(1)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(x, n_class):\n",
    "    \"\"\"One Hot encoding\n",
    "    \n",
    "    parameter\n",
    "    ---------\n",
    "    x: array_like [n_sample]\n",
    "    \n",
    "    n_class: number of class\n",
    "    \n",
    "    return\n",
    "    ------\n",
    "    en_1hot: array_like [n_smaple, n_class] one hot encoding matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    en_1hot = np.zeros([len(x), n_class])\n",
    "    \n",
    "    for idx, cat in enumerate(x):\n",
    "        en_1hot[idx, cat] = 1\n",
    "\n",
    "    return en_1hot\n",
    "\n",
    "\n",
    "def forward(X, keep_prob, layers):\n",
    "    \n",
    "    layer_fmt = 'layer_{}'\n",
    "    \n",
    "    A = X\n",
    "    for i in range(len(layers)):\n",
    "        W, b = layers[layer_fmt.format(i + 1)]\n",
    "        \n",
    "        with tf.name_scope(layer_fmt.format(i + 1)):\n",
    "            \n",
    "            Z = tf.matmul(A, W) + b\n",
    "            if i == len(layers) - 1:\n",
    "                A = tf.nn.sigmoid(Z)\n",
    "            else:\n",
    "                A = tf.nn.relu(Z)\n",
    "                A = tf.nn.dropout(A, keep_prob)\n",
    "            \n",
    "    return A\n",
    "\n",
    "\n",
    "def build_model(X, y_onehot, keep_prob, layers):\n",
    "        \n",
    "    y_output = forward(X, keep_prob, layers)\n",
    "    \n",
    "    with tf.name_scope('loss'):\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=y_onehot, logits=y_output)\n",
    "    cost_op = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    with tf.name_scope('pred'):\n",
    "         pred_op = tf.argmax(tf.nn.softmax(y_output), axis=-1) \n",
    "    \n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(y_onehot, 1), tf.argmax(y_output, 1))\n",
    "        correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "    acc_op = tf.reduce_mean(correct_prediction)\n",
    "    \n",
    "    return cost_op, pred_op, acc_op\n",
    "\n",
    "\n",
    "def next_batch(X, y, batch_size, shuffle=True):\n",
    "    \"\"\"Get next batch data\n",
    "    \"\"\"\n",
    "    \n",
    "    n_sample = X.shape[0]\n",
    "    n_batch = n_sample // batch_size\n",
    "    n_batch = n_batch + 1 if (n_sample % n_batch) != 0 else n_batch \n",
    "    idx = np.array(range(n_sample))\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "        \n",
    "    for b_idx in range(n_batch):\n",
    "        start, end = b_idx * batch_size, (b_idx + 1) * batch_size\n",
    "        if end >= n_sample:\n",
    "            sample_idx = idx[start:]\n",
    "        else:\n",
    "            sample_idx = idx[start:end] \n",
    "        \n",
    "        yield X[sample_idx, :], y[sample_idx, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_split_data(csv_file):\n",
    "    \"\"\"Split image and label from mnist csv data\n",
    "    \n",
    "    return\n",
    "    ------\n",
    "    X: array_like [n_sample, n_feature] mnist flat image\n",
    "    y: array_like [n_sample] mnist image label\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    X = df.iloc[:, 1:-1].values\n",
    "    y = df.iloc[:, -1].values\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = mnist_split_data('ex5_train.csv')\n",
    "X_test, y_test = mnist_split_data('ex5_test.csv')\n",
    "\n",
    "n_class = len(np.unique(y_train))\n",
    "y_train_en = one_hot_encode(y_train, n_class)\n",
    "y_test_en = one_hot_encode(y_test, n_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_initializer(n_inputs, n_outputs):\n",
    "    \n",
    "    init_range = tf.sqrt(6.0 / (n_inputs + n_outputs))\n",
    "    \n",
    "    return tf.random_uniform([n_inputs, n_outputs], -init_range, init_range)\n",
    "\n",
    "\n",
    "def weights_init(layer_dims):\n",
    "    \"\"\"weights initialize for layers\n",
    "    \n",
    "    parameter\n",
    "    ---------\n",
    "    layer_dims: list [n_input, n_output]\n",
    "    \n",
    "    return\n",
    "    ------\n",
    "    layers: dictionary of layer weights\n",
    "    \"\"\"\n",
    "    \n",
    "    layers = {}\n",
    "    layer_fmt = 'layer_{}'\n",
    "    var_fmt = '{var}{num}'\n",
    "    \n",
    "    for idx, (n_inputs, n_outputs) in enumerate(layer_dims):\n",
    "        layer_name = layer_fmt.format(idx + 1)\n",
    "        \n",
    "        with tf.name_scope(layer_name):\n",
    "            weight = tf.Variable(xavier_initializer(n_inputs, n_outputs), \n",
    "                             name=var_fmt.format(var='W', num=idx + 1))\n",
    "\n",
    "            bias = tf.Variable(tf.zeros([n_outputs]), \n",
    "                           name=var_fmt.format(var='b', num=idx + 1))\n",
    "    \n",
    "        layers[layer_name] = (weight, bias)\n",
    "        \n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Network model with 2 hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_dims = [(400, 256),   # (n_input, n_output)\n",
    "#               (256, 256),\n",
    "#               (256, 128),\n",
    "#               (128,  10)]\n",
    "\n",
    "layer_dims = [(400, 256),   # (n_input, n_output)\n",
    "              (256, 256),\n",
    "              (256,  10)]\n",
    "\n",
    "dropout_rate = 0.2\n",
    "\n",
    "# prepare variables\n",
    "layers = weights_init(layer_dims)\n",
    "X = tf.placeholder(tf.float32, [None, 400], name='X')\n",
    "y_onehot = tf.placeholder(tf.float32, [None, 10], name='y_onehot')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "# build model\n",
    "cost_op, pred_op, acc_op = build_model(X, y_onehot, keep_prob, layers)\n",
    "train_op = tf.train.AdamOptimizer().minimize(cost_op)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch setting\n",
    "epochs = 30\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "        \n",
    "    costs = []    \n",
    "    for X_batch, y_batch in next_batch(X_train, y_train_en, batch_size, shuffle=True):\n",
    "    \n",
    "        _, cost = sess.run([train_op, cost_op], \n",
    "                            feed_dict={ X: X_batch, y_onehot: y_batch, \n",
    "                                        keep_prob: 1 - dropout_rate})\n",
    "        costs.append(cost)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0: \n",
    "        acc = sess.run(acc_op, feed_dict={ X: X_train, y_onehot: y_train_en, \n",
    "                                           keep_prob: 1.0})\n",
    "        print('epoch:[{:3d}/{:3d}] avg cost: {:6.4f} acc: {:4.4f}'.\n",
    "              format(epoch + 1, epochs, np.mean(costs), acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = sess.run(acc_op, feed_dict={ X: X_train, y_onehot: y_train_en, \n",
    "                                         keep_prob: 1.0})\n",
    "print(\"train acc: {:4.4f}\".format(train_acc))\n",
    "\n",
    "test_acc = sess.run(acc_op, feed_dict={ X: X_test, y_onehot: y_test_en,\n",
    "                                        keep_prob: 1.0})\n",
    "print(\"test acc: {:4.4f}\".format(test_acc))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_dims = [(400, 256),   # (n_input, n_output)\n",
    "#               (256, 256),\n",
    "#               (256, 128),\n",
    "#               (128,  10)]\n",
    "\n",
    "layer_dims = [(400, 256),   # (n_input, n_output)\n",
    "              (256, 256),\n",
    "              (256,  10)]\n",
    "\n",
    "layers = weights_init(layer_dims)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 400], name='X')\n",
    "y_onehot = tf.placeholder(tf.float32, [None, 10], name='y_onehot')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "cost_op, pred_op, acc_op = build_model(X, y_onehot, keep_prob, layers)\n",
    "train_op = tf.train.AdamOptimizer().minimize(cost_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch setting\n",
    "epochs = 30\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rates = np.arange(0.1, 1.0, 0.1)\n",
    "train_accs = []\n",
    "test_accs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dropout_rate in dropout_rates:\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            costs = 0\n",
    "            for X_batch, y_batch in next_batch(X_train, y_train_en, batch_size, shuffle=True):\n",
    "                \n",
    "                _ = sess.run(train_op, \n",
    "                             feed_dict={ X: X_batch, y_onehot: y_batch, \n",
    "                                         keep_prob: 1 - dropout_rate})\n",
    "\n",
    "        train_acc = sess.run(acc_op, feed_dict={ X: X_train, y_onehot: y_train_en,\n",
    "                                                 keep_prob: 1.0})\n",
    "        test_acc = sess.run(acc_op, feed_dict={ X: X_test, y_onehot: y_test_en, \n",
    "                                                keep_prob: 1.0})\n",
    "        \n",
    "        train_accs.append(train_acc)\n",
    "        test_accs.append(test_acc)\n",
    "        \n",
    "        print('dropout rate: {:.2f} train acc: {:4.4f} test acc: {:4.4f}'.\n",
    "              format(dropout_rate, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dropout_rates, train_accs, label='train acc', color='b', marker='o')\n",
    "plt.plot(dropout_rates, test_accs, label='test_acc', color='r', marker='o')\n",
    "plt.title('Dropout rate vs accuracy')\n",
    "plt.xlabel('Dropout rate')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
