{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(x, n_class):\n",
    "    \"\"\"One Hot encoding\n",
    "    \n",
    "    parameter\n",
    "    ---------\n",
    "    x: array_like [n_sample]\n",
    "    \n",
    "    n_class: number of class\n",
    "    \n",
    "    return\n",
    "    ------\n",
    "    en_1hot: array_like [n_smaple, n_class] one hot encoding matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    en_1hot = np.zeros([len(x), n_class])\n",
    "    \n",
    "    for idx, cat in enumerate(x):\n",
    "        en_1hot[idx, cat] = 1\n",
    "\n",
    "    return en_1hot\n",
    "\n",
    "\n",
    "def sigmoid(x, derivative=False):\n",
    "    if derivative:\n",
    "        return x * (1. - x)\n",
    "    \n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "def relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        return 1.0 * (x > 0)\n",
    "    #x[x < 0] = 0\n",
    "    return x * (x > 0)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return  exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "def foward(X, params, is_training=True):\n",
    "    \"\"\"Neuron network forward\n",
    "    \n",
    "    parameter\n",
    "    ----------\n",
    "    X: array_like [n_sample, n_input]\n",
    "    params: dictionary of layer weights\n",
    "    \n",
    "    return\n",
    "    ------\n",
    "    output: final layer output\n",
    "    caches: dictionary of foward layer output\n",
    "    \"\"\"\n",
    "\n",
    "    caches = {}\n",
    "    n_layers = len(params)\n",
    "    layer_fmt = 'layer_{}'\n",
    "    \n",
    "    A = X\n",
    "    for idx in range(n_layers):\n",
    "        W, b, keep_prob = params[layer_fmt.format(idx + 1)]\n",
    "        Z = np.matmul(A, W) + b\n",
    "        \n",
    "        # last layer\n",
    "        if idx == n_layers - 1:\n",
    "            A = sigmoid(Z)\n",
    "        else:\n",
    "            A = relu(Z)\n",
    "            \n",
    "        if is_training and keep_prob is not None:\n",
    "            D =  np.random.binomial(1, keep_prob, size=A.shape) / (keep_prob)\n",
    "            A = A * D\n",
    "            caches[layer_fmt.format(idx + 1)] = (Z, A, D)\n",
    "        else:\n",
    "            caches[layer_fmt.format(idx + 1)] = (Z, A, None)\n",
    "    \n",
    "    # padding input layer to cache\n",
    "    caches[layer_fmt.format(0)] = (None, X, None)\n",
    "\n",
    "    return A, caches\n",
    "    \n",
    "\n",
    "def backward(X, y, params, caches, is_traning=True):\n",
    "    \"\"\"Neuron network backward\n",
    "    \n",
    "    parameter\n",
    "    ----------\n",
    "    X: array_like [n_sample, n_input]\n",
    "    y: array_like [n_sample, n_class]\n",
    "    params: dictionary of layer weights\n",
    "    caches: dictionary of foward layer output\n",
    "    \n",
    "    return\n",
    "    ------\n",
    "    grads: dictionary of layer weights gradient\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    n_layers = len(params)\n",
    "    layer_fmt = 'layer_{}'\n",
    "    \n",
    "    # last layer gradient\n",
    "    _, A, _ = caches[layer_fmt.format(n_layers)]\n",
    "    dZ = A - y\n",
    "    \n",
    "    grads = {}\n",
    "    gnorm = 0\n",
    "    for idx in reversed(xrange(n_layers)):\n",
    "        W, _, _ = params[layer_fmt.format(idx + 1)]\n",
    "        _, A, D = caches[layer_fmt.format(idx)]\n",
    "                \n",
    "        dW = np.dot(A.T, dZ) / m \n",
    "        dW_reg = W / m\n",
    "        db = np.sum(dZ, axis=0) / m\n",
    "        \n",
    "        dZ = np.dot(dZ, W.T) * relu(A, derivative=True) # (A * (1. - A))\n",
    "     \n",
    "        if D is not None:\n",
    "            dZ = dZ * D\n",
    "    \n",
    "        gnorm += np.linalg.norm(dW) + np.linalg.norm(dW_reg) + np.linalg.norm(db)\n",
    "        grads[layer_fmt.format(idx + 1)] = (dW, db, dW_reg)\n",
    "    \n",
    "    return grads, gnorm\n",
    "\n",
    "\n",
    "def update_parameters(params, grads, alpha, learning_rate):\n",
    "    \"\"\"Update parameters\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    grads: dictionary of layer weights gradient\n",
    "    params: dictionary of layer weights\n",
    "    learning_rate: learing rate to update weights\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    n_layers = len(grads)\n",
    "    layer_fmt = 'layer_{}'\n",
    "    \n",
    "    for idx in xrange(n_layers):\n",
    "        W, b, _ = params[layer_fmt.format(idx + 1)]\n",
    "        dW, db, dW_reg = grads[layer_fmt.format(idx + 1)]\n",
    "        W[:] = W - learning_rate * (dW + alpha * dW_reg)\n",
    "        b[:] = b - learning_rate * db\n",
    "    \n",
    "    return params\n",
    "\n",
    "\n",
    "def compute_cost(output, y):\n",
    "    \"\"\"Compute loss\n",
    "    \n",
    "    parameter\n",
    "    ---------\n",
    "    output: array_like [n_sample, n_class] nn predict output\n",
    "    y: array_like [n_sample, n_class]\n",
    "    \"\"\"\n",
    "    loss = y * np.log(output) + (1 - y) * np.log(1 - output)\n",
    "    loss = -np.mean(np.sum(loss, axis=-1, keepdims=True))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def comput_reg_cost(params):\n",
    "    \n",
    "    reg_cost = 0\n",
    "    layer_fmt = 'layer_{}'\n",
    "    for idx in xrange(len(params)):\n",
    "        W, _, _ = params[layer_fmt.format(idx + 1)]\n",
    "        reg_cost += np.linalg.norm(W)\n",
    "    \n",
    "    return reg_cost\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, params, alpha=1e-3, learning_rate=1e-2,\n",
    "                     max_iter=None, gtol=1e-5,\n",
    "                     show_cost=None):\n",
    "    \"\"\"Gradient descent \n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    X: array_like [n_samples, n_features]\n",
    "    y: array_like [n_samples, n_class]\n",
    "    alpha: l2 regularization\n",
    "    params: dictionary of layer weights\n",
    "\n",
    "    return\n",
    "    ------\n",
    "    parmas: dictionary of layer weights\n",
    "    costs: costs of iteratsions\n",
    "    \"\"\"\n",
    "    \n",
    "    if max_iter is None:\n",
    "        max_iter = 1000\n",
    "    \n",
    "    n_sample = X.shape[0]\n",
    "    costs = np.zeros(max_iter)\n",
    "    \n",
    "    for n_iter in xrange(max_iter):    \n",
    "        \n",
    "        output, caches = foward(X, params)\n",
    "        costs[n_iter] = compute_cost(output, y) + (comput_reg_cost(params) * (alpha / (2. * n_sample)))\n",
    "        grads, gnorm = backward(X, y, params, caches)\n",
    "        params = update_parameters(params, grads, alpha, learning_rate)\n",
    "        \n",
    "        if show_cost is not None and (n_iter % show_cost) == 0:\n",
    "            print('costs[%4d]: %.4e' % (n_iter, costs[n_iter]))\n",
    "            \n",
    "        if gnorm < gtol:\n",
    "            costs = costs[:n_iter]\n",
    "            break\n",
    "    \n",
    "    return params, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_split_data(csv_file):\n",
    "    \"\"\"Split image and label from mnist csv data\n",
    "    \n",
    "    return\n",
    "    ------\n",
    "    X: array_like [n_sample, n_feature] mnist flat image\n",
    "    y: array_like [n_sample] mnist image label\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    X = df.iloc[:, 1:-1].values\n",
    "    y = df.iloc[:, -1].values\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = mnist_split_data('exam1_train.csv')\n",
    "X_test, y_test = mnist_split_data('exam1_test.csv')\n",
    "\n",
    "n_class = len(np.unique(y_train))\n",
    "y_train_en = one_hot_encode(y_train, n_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(layer_dims):\n",
    "    \"\"\"weights initialize for layers\n",
    "    \n",
    "    parameter\n",
    "    ---------\n",
    "    layer_dims: list [n_input, n_output]\n",
    "    \n",
    "    return\n",
    "    ------\n",
    "    model: dictionary of layer weights\n",
    "    \"\"\"\n",
    "    \n",
    "    model = {}\n",
    "    \n",
    "    for idx, (n_input, n_output, dropout_rate) in enumerate(layer_dims):\n",
    "        layer_name = 'layer_{}'.format(idx + 1)\n",
    "        \n",
    "        # He initializer \n",
    "        weight = np.random.randn(n_input, n_output) / np.sqrt(n_input / 2)\n",
    "        #bias = np.zeros([n_output])\n",
    "        bias = np.ones([n_output]) * 0.01\n",
    "    \n",
    "        model[layer_name] = (weight, bias, dropout_rate)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Network model with 2 hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [(400, 256,   1.),      # (n_input, n_output, keep_prob)\n",
    "              (256,  32,   1.),\n",
    "              ( 32,  10, None)]\n",
    "\n",
    "model = weights_init(layer_dims)\n",
    "\n",
    "n_sample = X_train.shape[0]\n",
    "epochs = 30\n",
    "batch_size = 128\n",
    "n_batch = n_sample // batch_size\n",
    "n_batch = n_batch + 1 if (n_sample % n_batch) != 0 else n_batch \n",
    "idx = np.array(range(n_sample))\n",
    "\n",
    "for epoch in xrange(epochs):\n",
    "    #print('epoch:[{:2d}/{:2d}]\\r'.format(epoch + 1, epochs))\n",
    "    np.random.shuffle(idx)\n",
    "    for b_idx in xrange(n_batch):\n",
    "        start, end = b_idx * batch_size, (b_idx + 1) * batch_size\n",
    "        if end >= n_sample:\n",
    "            sample_idx = idx[start:]\n",
    "        else:\n",
    "            sample_idx = idx[start:end]\n",
    "        \n",
    "        model, costs = gradient_descent(X_train[sample_idx,:], y_train_en[sample_idx, :], model,\n",
    "                                        learning_rate=0.1, max_iter=1)\n",
    "\n",
    "# model, costs = gradient_descent(X_train, y_train_en, model, alpha=1e-2,\n",
    "#                                 learning_rate=0.1, max_iter=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, model):\n",
    "    \"\"\"Predcit smaples with softmax prob\n",
    "    \n",
    "    parameter\n",
    "    ---------\n",
    "    X: array_like [n_sample, n_feature]\n",
    "    model: dictionary of layer weights\n",
    "    \n",
    "    return\n",
    "    ------\n",
    "    softmax prob label \n",
    "    \"\"\"\n",
    "    \n",
    "    output, _ = foward(X, model, is_training=False)\n",
    "    \n",
    "    return np.argmax(softmax(output), axis=-1)\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    \n",
    "    return np.mean(np.equal(y_pred, y_true))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 98.0571\n",
      "test acc: 91.6667\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = predict(X_train, model)\n",
    "train_acc = accuracy(y_train_pred, y_train)\n",
    "print(\"train acc: {:6.4f}\".format(train_acc * 100))\n",
    "\n",
    "y_test_pred = predict(X_test, model)\n",
    "test_acc = accuracy(y_test_pred, y_test)\n",
    "print(\"test acc: {:6.4f}\".format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_weights(dst, src):\n",
    "\n",
    "    layer_fmt = 'layer_{}'\n",
    "    for idx in xrange(len(src)):\n",
    "        W_src, b_src, _ = src[layer_fmt.format(idx + 1)]\n",
    "        W_dst, b_dst, _ = dst[layer_fmt.format(idx + 1)]\n",
    "        W_dst[:] = W_src\n",
    "        b_dst[:] = b_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsungmin/anaconda3/envs/python2/lib/python2.7/site-packages/ipykernel_launcher.py:159: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/tsungmin/anaconda3/envs/python2/lib/python2.7/site-packages/ipykernel_launcher.py:159: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.2500 alpha 0.0100 epochs 104\n",
      "Train acc: 100.00\n",
      "Test acc: 95.13\n"
     ]
    }
   ],
   "source": [
    "# layer_dims = [(400, 512, 0.5),      # (n_input, n_output, keep_prob)\n",
    "#               (512, 256, 0.5),      # lr = 0.3\n",
    "#               (256,  64, 0.5),      # epoch = 150\n",
    "#               ( 64,  32, 0.5),      # batch = 256\n",
    "#               ( 32,  10, None)]\n",
    "\n",
    "# layer_dims = [(400, 256, 0.8),      # (n_input, n_output, keep_prob)\n",
    "#               (256, 256, 0.8),      # lr = 0.5\n",
    "#               (256,  64, 0.8),      # alpha = 1e-3\n",
    "#               ( 64,  32, 0.8),      # epoch = 150 \n",
    "#               ( 32,  10, None)]     # batch = 128\n",
    "\n",
    "layer_dims = [(400, 256, 0.5),\n",
    "              (256, 256,None),\n",
    "              (256, 128, 0.5),\n",
    "              (128,  10,None)]\n",
    "\n",
    "#n_iter = 1000\n",
    "learning_rate = 0.25 #0.15\n",
    "alpha = 1e-2\n",
    "\n",
    "n_sample = X_train.shape[0]\n",
    "epochs = 150\n",
    "batch_size = 128\n",
    "n_batch = n_sample // batch_size\n",
    "n_batch = n_batch + 1 if (n_sample % n_batch) != 0 else n_batch \n",
    "idx = np.array(range(n_sample))\n",
    "\n",
    "model = weights_init(layer_dims)\n",
    "best_model = weights_init(layer_dims)\n",
    "best_acc = 0.\n",
    "\n",
    "for epoch in xrange(epochs):\n",
    "    np.random.shuffle(idx)\n",
    "    for b_idx in xrange(n_batch):\n",
    "        start, end = b_idx * batch_size, (b_idx + 1) * batch_size\n",
    "        if end >= n_sample:\n",
    "            sample_idx = idx[start:]\n",
    "        else:\n",
    "            sample_idx = idx[start:end]\n",
    "        \n",
    "        model, costs = gradient_descent(X_train[sample_idx,:], y_train_en[sample_idx, :], model, alpha=alpha,\n",
    "                                        learning_rate=learning_rate, max_iter=1)\n",
    "    \n",
    "    acc = accuracy(predict(X_test, model), y_test)\n",
    "    #print('epoch:[{:3d}/{:3d}] acc: {:4.2f}'.format(epoch + 1, epochs, acc * 100))\n",
    "    if acc > 0.95:\n",
    "        break\n",
    "        #best_acc = acc\n",
    "        #copy_weights(best_model, model)b\n",
    "\n",
    "y_train_pred = predict(X_train, model)\n",
    "train_acc = accuracy(y_train_pred, y_train)   \n",
    "    \n",
    "y_test_pred = predict(X_test, model)\n",
    "test_acc = accuracy(y_test_pred, y_test)\n",
    "\n",
    "print('learning rate {:.4f} alpha {:.4f} epochs {:d}'.format(learning_rate, alpha, epoch))\n",
    "print('Train acc: {:4.2f}'.format(train_acc * 100))\n",
    "print('Test acc: {:4.2f}'.format(test_acc * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
